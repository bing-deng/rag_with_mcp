"""
å®Œå…¨ä¿®å¤ç‰ˆå¼ºåŒ–RAGæœåŠ¡ - ç»Ÿä¸€å‘é‡ç©ºé—´
"""
import sys
import os

# é¡¹ç›®è·¯å¾„è®¾ç½®
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
weaviate_path = os.path.join(project_root, 'weaviate')
sys.path.insert(0, weaviate_path)

from bedrock.bedrock_usage import TokyoBedrockService
from weaviate_client import WeaviateRAGClient
from pdf_processor import PDFProcessor
from typing import List, Dict, Any, Optional
import time
import re

class EnhancedWebRAGService:
    """å®Œå…¨ä¿®å¤ç‰ˆå¼ºåŒ–RAGæœåŠ¡ - ç»Ÿä¸€å‘é‡ç©ºé—´å¤„ç†"""
    
    def __init__(self):
        print("ğŸ”§ åˆå§‹åŒ–å®Œå…¨ä¿®å¤ç‰ˆå¼ºåŒ–RAGæœåŠ¡...")
        self.bedrock_service = TokyoBedrockService()
        self.weaviate_client = WeaviateRAGClient()
        self.pdf_processor = PDFProcessor()
        self.collection_name = "FixedDocumentCollection"
        
        # æŸ¥è¯¢æ‰©å±•è¯å…¸
        self.query_expansions = {
            'é›»åœ§èª¿æŸ»': ['é›»åœ§ç•°å¸¸', 'é›»åœ§æ¸¬å®š', 'é›»åœ§ç¢ºèª', 'é›»åœ§ãƒã‚§ãƒƒã‚¯'],
            'è¨ˆå™¨ç•ªå·': ['è¨ˆé‡å™¨ç•ªå·', 'ãƒ¡ãƒ¼ã‚¿ãƒ¼ç•ªå·', 'è¨ˆé‡å™¨'],
            'ä¾›çµ¦åœ°ç‚¹': ['ä¾›çµ¦åœ°ç‚¹ç‰¹å®šç•ªå·', 'éœ€è¦å ´æ‰€', 'ä¾›çµ¦ç®‡æ‰€'],
            'é›»æŸ±ç•ªå·': ['é›»æŸ±', 'æŸ±ç•ªå·', 'ãƒãƒ¼ãƒ«ç•ªå·'],
            'æƒ…å ±åé›†': ['èãå–ã‚Š', 'ç¢ºèªé …ç›®', 'èª¿æŸ»å†…å®¹', 'ãƒ‡ãƒ¼ã‚¿åé›†'],
            'å„ªå…ˆçš„': ['é‡è¦', 'å¿…é ˆ', 'å„ªå…ˆ', 'ã¾ãš'],
            '4ã¤ã®æƒ…å ±': ['4é …ç›®', '4ã¤ã®é …ç›®', 'é‡è¦äº‹é …', 'èª¿æŸ»é …ç›®'],
        }
        
        print("âœ… å®Œå…¨ä¿®å¤ç‰ˆRAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def initialize_knowledge_base(self, pdf_path: str) -> bool:
        """ç»Ÿä¸€å‘é‡ç©ºé—´çš„çŸ¥è¯†åº“åˆå§‹åŒ–"""
        try:
            print("ğŸ“š å¼€å§‹ç»Ÿä¸€å‘é‡ç©ºé—´çŸ¥è¯†åº“åˆå§‹åŒ–...")
            
            # 1. Weaviateè¿æ¥
            if not self.weaviate_client.wait_for_weaviate(timeout=60):
                print("âŒ Weaviateè¿æ¥å¤±è´¥")
                return False
            
            # 2. å¤„ç†PDFæ–‡æ¡£
            print("ğŸ“„ å¤„ç†PDFæ–‡æ¡£...")
            chunks = self.pdf_processor.process_pdf(pdf_path)
            
            if not chunks:
                print("âŒ PDFå¤„ç†å¤±è´¥")
                return False
            
            print(f"âœ… PDFå¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")
            
            # ğŸ¯ å…³é”®æ­¥éª¤ï¼šç»Ÿä¸€ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£çš„CohereåµŒå…¥
            print("ğŸ§  ç»Ÿä¸€ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡ï¼ˆCohere multilingual v3ï¼‰...")
            doc_texts = [chunk['content'] for chunk in chunks]
            doc_embeddings = self.bedrock_service.get_embeddings(
                doc_texts, 
                input_type="search_document"
            )
            
            if not doc_embeddings or len(doc_embeddings) != len(chunks):
                print(f"âŒ æ–‡æ¡£åµŒå…¥ç”Ÿæˆå¤±è´¥: éœ€è¦{len(chunks)}ä¸ªï¼Œè·å¾—{len(doc_embeddings) if doc_embeddings else 0}ä¸ª")
                return False
            
            print(f"âœ… æˆåŠŸç”Ÿæˆæ–‡æ¡£åµŒå…¥: {len(doc_embeddings)} ä¸ªï¼Œç»´åº¦: {len(doc_embeddings[0])}")
            
            # 3. åˆ›å»ºæ— vectorizerçš„schema
            print("ğŸ—ƒï¸ åˆ›å»ºå¤–éƒ¨åµŒå…¥ä¸“ç”¨å‘é‡æ•°æ®åº“...")
            if not self.weaviate_client.create_schema(self.collection_name):
                print("âŒ Schemaåˆ›å»ºå¤±è´¥")
                return False
            
            # 4. ğŸ¯ å…³é”®æ­¥éª¤ï¼šä½¿ç”¨å¤–éƒ¨åµŒå…¥æ·»åŠ æ–‡æ¡£
            print("ğŸ“ ä½¿ç”¨å¤–éƒ¨åµŒå…¥æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
            if not self.weaviate_client.add_documents_with_external_vectors(
                chunks, doc_embeddings, self.collection_name
            ):
                print("âŒ å¤–éƒ¨åµŒå…¥æ–‡æ¡£æ·»åŠ å¤±è´¥")
                return False
            
            # 5. éªŒè¯æ•°æ®
            time.sleep(5)  # ç¨ç­‰ç‰‡åˆ»è®©æ•°æ®ç´¢å¼•å®Œæˆ
            count = self.weaviate_client.get_stats(self.collection_name)
            if count > 0:
                print(f"âœ… ç»Ÿä¸€å‘é‡ç©ºé—´çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼åŒ…å« {count} ä¸ªæ–‡æ¡£å—")
                return True
            else:
                print("âŒ çŸ¥è¯†åº“éªŒè¯å¤±è´¥ï¼Œæ–‡æ¡£æ•°é‡ä¸º0")
                return False
                
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def unified_semantic_search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ç»Ÿä¸€å‘é‡ç©ºé—´çš„è¯­ä¹‰æœç´¢"""
        try:
            print(f"ğŸ” æ‰§è¡Œç»Ÿä¸€å‘é‡ç©ºé—´è¯­ä¹‰æœç´¢: {query}")
            
            # ğŸ¯ å…³é”®æ­¥éª¤ï¼šä½¿ç”¨ç›¸åŒCohereæ¨¡å‹ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embeddings = self.bedrock_service.get_embeddings(
                [query], 
                input_type="search_query"
            )
            
            if not query_embeddings or len(query_embeddings) == 0:
                print("âŒ æŸ¥è¯¢åµŒå…¥ç”Ÿæˆå¤±è´¥")
                return []
            
            query_embedding = query_embeddings[0]
            print(f"âœ… æŸ¥è¯¢åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")
            
            # ğŸ¯ å…³é”®æ­¥éª¤ï¼šä½¿ç”¨å¤–éƒ¨åµŒå…¥å‘é‡è¿›è¡Œæœç´¢
            results = self.weaviate_client.semantic_search_with_external_vector(
                query_embedding, 
                class_name=self.collection_name, 
                limit=limit
            )
            
            if results:
                print(f"âœ… ç»Ÿä¸€ç©ºé—´æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªæ–‡æ¡£")
                for i, doc in enumerate(results[:3], 1):
                    print(f"  {i}. ç›¸ä¼¼åº¦: {doc.get('certainty', 0):.3f}")
                    print(f"     å†…å®¹é¢„è§ˆ: {doc['content'][:80]}...")
            
            return results
            
        except Exception as e:
            print(f"âŒ ç»Ÿä¸€è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def multi_query_search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """å¤šæŸ¥è¯¢ç­–ç•¥æœç´¢"""
        try:
            all_results = []
            seen_contents = set()
            
            # åŸæŸ¥è¯¢
            results1 = self.unified_semantic_search(query, limit)
            self.add_unique_results(results1, all_results, seen_contents, "åŸæŸ¥è¯¢")
            
            # æ‰©å±•æŸ¥è¯¢
            expanded_query = self.expand_query(query)
            if expanded_query != query:
                results2 = self.unified_semantic_search(expanded_query, limit)
                self.add_unique_results(results2, all_results, seen_contents, "æ‰©å±•æŸ¥è¯¢")
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            all_results.sort(key=lambda x: x.get('certainty', 0), reverse=True)
            return all_results[:limit]
            
        except Exception as e:
            print(f"âŒ å¤šæŸ¥è¯¢æœç´¢å¤±è´¥: {e}")
            return []
    
    def add_unique_results(self, results: List[Dict], all_results: List[Dict], 
                          seen_contents: set, search_method: str):
        """å»é‡æ·»åŠ ç»“æœ"""
        for result in results:
            content_key = result['content'][:100]
            if content_key not in seen_contents:
                seen_contents.add(content_key)
                result['search_method'] = search_method
                all_results.append(result)
    
    def expand_query(self, query: str) -> str:
        """æŸ¥è¯¢æ‰©å±•"""
        expanded = query
        for key, synonyms in self.query_expansions.items():
            if key in query:
                expanded += f" {' '.join(synonyms)}"
        
        if '4ã¤' in query:
            expanded += " å››ã¤ 4é …ç›® 4å€‹"
        
        return expanded
    
    def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> Optional[str]:
        """ç”Ÿæˆç²¾å‡†å›ç­”"""
        try:
            if not context_docs:
                return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãŠå°‹ã­ã®å†…å®¹ã«é–¢ã™ã‚‹æƒ…å ±ãŒè³‡æ–™ã‹ã‚‰è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            print(f"\nğŸ“‹ å›ç­”ç”Ÿæˆç”¨æ–‡æ›¸ ({len(context_docs)}ä»¶):")
            for i, doc in enumerate(context_docs, 1):
                print(f"  {i}. ç›¸ä¼¼åº¦: {doc.get('certainty', 0):.3f}")
                print(f"     å†…å®¹: {doc['content'][:100]}...")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            formatted_docs = []
            for i, doc in enumerate(context_docs, 1):
                certainty = doc.get('certainty', 0)
                search_method = doc.get('search_method', 'çµ±ä¸€æ¤œç´¢')
                
                formatted_doc = f"[æ–‡æ›¸{i}] (ç›¸ä¼¼åº¦: {certainty:.3f}, æ¤œç´¢æ–¹æ³•: {search_method})\n{doc['content']}"
                formatted_docs.append(formatted_doc)
            
            search_results = "\n\n".join(formatted_docs)
            
            if len(search_results) > 4500:
                search_results = search_results[:4500] + "\n\n[æ³¨: å†…å®¹ãŒé•·ã„ãŸã‚ä¸€éƒ¨çœç•¥]"
            
            # é’ˆå¯¹é¡¹ç›®åˆ—ä¸¾é—®é¢˜çš„ç‰¹åŒ–æç¤º
            if any(keyword in query for keyword in ['4ã¤', '4é …ç›®', 'å„ªå…ˆçš„', 'åé›†']):
                enhanced_prompt = f"""ã‚ãªãŸã¯é›»åŠ›è¨­å‚™ã®å°‚é–€ã‚¹ã‚¿ãƒƒãƒ•ã§ã™ã€‚ä»¥ä¸‹ã®è³‡æ–™ã«åŸºã¥ã„ã¦è³ªå•ã«æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

--- è³‡æ–™æ¤œç´¢çµæœ ---
{search_results}
--- æ¤œç´¢çµæœçµ‚äº† ---

è³ªå•: {query}

é‡è¦äº‹é …:
1. è³‡æ–™ã®å†…å®¹ã®ã¿ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„
2. å…·ä½“çš„ãªé …ç›®ãŒã‚ã‚‹å ´åˆã¯ã€å¿…ãšç•ªå·ä»˜ãã§æ˜ç¢ºã«åˆ—æŒ™ã—ã¦ãã ã•ã„
3. ä¾‹ç¤ºãŒã‚ã‚‹å ´åˆã¯å…·ä½“çš„ã«è¨˜è¼‰ã—ã¦ãã ã•ã„
4. æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œè³‡æ–™ã«è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„

å›ç­”å½¢å¼:
**å›ç­”:**
[è³‡æ–™ã«åŸºã¥ãå…·ä½“çš„ãªé …ç›®ã‚„å†…å®¹]

**è©³ç´°:**
[å¿…è¦ã«å¿œã˜ã¦å„é …ç›®ã®èª¬æ˜]

**å‚è€ƒ:**
[è³‡æ–™ã‹ã‚‰ã®å…·ä½“çš„ãªå¼•ç”¨]"""
            else:
                enhanced_prompt = f"""ä»¥ä¸‹ã®è³‡æ–™ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

--- è³‡æ–™æ¤œç´¢çµæœ ---
{search_results}
--- æ¤œç´¢çµæœçµ‚äº† ---

è³ªå•: {query}

è³‡æ–™ã®å†…å®¹ã®ã¿ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã§å®Ÿç”¨çš„ãªå›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚"""
            
            print("ğŸ¤– ç²¾å‡†å›ç­”ç”Ÿæˆä¸­...")
            
            answer = self.bedrock_service.chat_with_claude(
                message=enhanced_prompt,
                system_prompt="è³‡æ–™ã«åŸºã¥ãæ­£ç¢ºãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚æ¨æ¸¬ã‚„æƒ³åƒã¯ç¦æ­¢ã§ã™ã€‚å…·ä½“çš„ãªé …ç›®ã¯æ˜ç¢ºã«åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚",
                max_tokens=2000
            )
            
            if answer:
                print("âœ… ç²¾å‡†å›ç­”ç”ŸæˆæˆåŠŸ")
                return answer
            else:
                return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã„ãŸã—ã¾ã—ãŸã€‚"
                
        except Exception as e:
            print(f"âŒ å›ç­”ç”Ÿæˆé”™è¯¯: {e}")
            return "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã„ãŸã—ã¾ã—ãŸã€‚"
    
    def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """å®Œæ•´çš„ç»Ÿä¸€å‘é‡ç©ºé—´æŸ¥è¯¢"""
        print(f"\n{'='*80}")
        print(f"ğŸ” çµ±ä¸€ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã‚¯ã‚¨ãƒªå®Ÿè¡Œ: {question}")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        # å¤šæŸ¥è¯¢ç­–ç•¥æœç´¢
        relevant_docs = self.multi_query_search(question, limit=top_k)
        
        # ç”Ÿæˆå›ç­”
        answer = self.generate_answer(question, relevant_docs)
        
        processing_time = time.time() - start_time
        
        result = {
            "question": question,
            "answer": answer,
            "sources": relevant_docs,
            "source_count": len(relevant_docs),
            "processing_time": round(processing_time, 2),
            "vector_space": "unified_cohere_external"
        }
        
        print(f"\nâœ… çµ±ä¸€ç©ºé–“ã‚¯ã‚¨ãƒªå®Œäº†ï¼Œå‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"{'='*80}")
        
        return result
    
    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            if hasattr(self.weaviate_client, 'close'):
                self.weaviate_client.close()
            print("âœ… RAGæœåŠ¡è¿æ¥å·²å…³é—­")
        except Exception as e:
            print(f"âš ï¸ å…³é—­è¿æ¥æ—¶è­¦å‘Š: {e}")

def test_unified_rag():
    """æµ‹è¯•ç»Ÿä¸€å‘é‡ç©ºé—´RAG"""
    pdf_path = os.path.join(os.path.dirname(__file__), "../bedrock/pdf/high_takusoukun_web_manual_separate.pdf")
    
    if not os.path.exists(pdf_path):
        print(f"âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_path}")
        return
    
    rag_service = None
    try:
        rag_service = EnhancedWebRAGService()
        
        # ç»Ÿä¸€å‘é‡ç©ºé—´çŸ¥è¯†åº“åˆå§‹åŒ–
        print("ğŸš€ çµ±ä¸€ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–é–‹å§‹...")
        if not rag_service.initialize_knowledge_base(pdf_path):
            print("âŒ çµ±ä¸€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å¤±æ•—")
            return
        
        # å…³é”®æµ‹è¯•é—®é¢˜
        critical_question = "é›»åœ§èª¿æŸ»ã§ã¯ã€ã©ã®4ã¤ã®æƒ…å ±ã‚’å„ªå…ˆçš„ã«åé›†ã™ã¹ãã§ã™ã‹ï¼Ÿ"
        
        print(f"\n{'='*100}")
        print("ğŸ¯ å…³é”®é—®é¢˜æµ‹è¯• - ç»Ÿä¸€å‘é‡ç©ºé—´ç‰ˆæœ¬")
        result = rag_service.query(critical_question)
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"é—®é¢˜: {result['question']}")
        print(f"æ£€ç´¢æ–‡æ¡£æ•°: {result['source_count']}")
        print(f"å¤„ç†æ—¶é—´: {result['processing_time']}ç§’")
        print(f"å‘é‡ç©ºé—´: {result['vector_space']}")
        print(f"\nğŸ¤– å›ç­”:")
        print(result['answer'])
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸå¾…çš„å…³é”®è¯
        answer_text = result['answer']
        expected_keywords = ['é›»æŸ±ç•ªå·', 'ä¸å…·åˆçŠ¶æ³', 'ç™ºç”Ÿæ™‚é–“å¸¯', 'ç™ºç”Ÿç¯„å›²']
        found_keywords = [kw for kw in expected_keywords if kw in answer_text]
        
        print(f"\nğŸ” å…³é”®è¯æ£€æŸ¥:")
        print(f"æœŸå¾…å…³é”®è¯: {expected_keywords}")
        print(f"å‘ç°å…³é”®è¯: {found_keywords}")
        
        if len(found_keywords) >= 3:
            print("ğŸ‰ SUCCESS: å…³é”®ä¿¡æ¯æ£€ç´¢æˆåŠŸï¼")
        else:
            print("âš ï¸ WARNING: éƒ¨åˆ†å…³é”®ä¿¡æ¯å¯èƒ½ç¼ºå¤±")
        
        print(f"{'='*100}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if rag_service:
            rag_service.close()

if __name__ == "__main__":
    test_unified_rag() 