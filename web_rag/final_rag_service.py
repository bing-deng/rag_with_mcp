"""
æœ€ç»ˆç‰ˆRAGæœåŠ¡ - ä¿®å¤å¯¼å…¥é—®é¢˜
"""
import sys
import os
import importlib

# å¼ºåˆ¶æ¸…é™¤ç¼“å­˜
if 'weaviate_client' in sys.modules:
    importlib.reload(sys.modules['weaviate_client'])

# ãƒ‘ã‚¹è¨­å®š
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
weaviate_path = os.path.join(project_root, 'weaviate')
sys.path.insert(0, weaviate_path)

from bedrock.bedrock_usage import TokyoBedrockService
from weaviate_client import WeaviateRAGClient  # ç›´æ¥ä»å½“å‰ç›®å½•å¯¼å…¥
from pdf_processor import PDFProcessor
from typing import List, Dict, Any, Optional
import time
import re

class FinalWebRAGService:
    """æœ€ç»ˆç‰ˆWebRAGã‚µãƒ¼ãƒ“ã‚¹ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self):
        print("ğŸ¯ æœ€çµ‚ç‰ˆRAGã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–ä¸­...")
        self.bedrock_service = TokyoBedrockService()
        self.weaviate_client = WeaviateRAGClient()
        self.pdf_processor = PDFProcessor()
        self.knowledge_loaded = False
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒ¡ã‚½ãƒƒãƒ‰å­˜åœ¨ç¢ºèª
        print(f"ğŸ”§ WeaviateRAGClient ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª:")
        methods = [method for method in dir(self.weaviate_client) if not method.startswith('_')]
        print(f"   åˆ©ç”¨å¯èƒ½ãƒ¡ã‚½ãƒƒãƒ‰: {', '.join(methods)}")
        
        # å…·ä½“çš„ã«ãƒã‚§ãƒƒã‚¯
        if hasattr(self.weaviate_client, 'add_documents_with_external_vectors'):
            print("âœ… add_documents_with_external_vectors ãƒ¡ã‚½ãƒƒãƒ‰å­˜åœ¨")
        else:
            print("âŒ add_documents_with_external_vectors ãƒ¡ã‚½ãƒƒãƒ‰å­˜åœ¨ã—ã¾ã›ã‚“")
    
    def load_pdf_knowledge(self, pdf_path: str, force_reload: bool = False):
        """PDFãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿ - ä¿®å¤ç‰ˆ"""
        try:
            # 1. PDFå‡¦ç†
            print("ğŸ“„ PDFå‡¦ç†é–‹å§‹...")
            chunks = self.pdf_processor.process_pdf(pdf_path)
            
            # 2. é›»åœ§èª¿æŸ»é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            voltage_keywords = [
                'é›»åœ§èª¿æŸ»', 'é›»åœ§ç•°å¸¸', 'é›»æŸ±ç•ªå·', 'ä¸å…·åˆçŠ¶æ³', 
                'ç™ºç”Ÿæ™‚é–“å¸¯', 'ç™ºç”Ÿç¯„å›²', 'è¨˜å…¥ã®ãƒã‚¤ãƒ³ãƒˆ',
                'é›»åœ§', 'èª¿æŸ»', 'å•åˆã›æƒ…å ±'
            ]
            
            relevant_chunks = []
            for i, chunk in enumerate(chunks):
                content = chunk['content'].lower()
                # ã‚ˆã‚Šå¯›å®¹ãªä¸€è‡´æ¡ä»¶
                if any(keyword.lower() in content for keyword in voltage_keywords):
                    chunk['chunk_id'] = f"chunk_{i}"
                    relevant_chunks.append(chunk)
            
            # 3. å®Œæ•´å›ç­”åŒ…å«æ£€æµ‹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            complete_answer_chunks = []
            for chunk in relevant_chunks:
                content = chunk['content']
                # ã‚ˆã‚Šå³å¯†ãªå®Œæ•´å›ç­”æ¤œå‡º
                required_all = ['é›»æŸ±ç•ªå·', 'ä¸å…·åˆçŠ¶æ³', 'ç™ºç”Ÿæ™‚é–“å¸¯', 'ç™ºç”Ÿç¯„å›²']
                has_all = all(req in content for req in required_all)
                
                if has_all and ('è¨˜å…¥ã®ãƒã‚¤ãƒ³ãƒˆ' in content or 'é›»åœ§èª¿æŸ»ã«ã¤ã„ã¦' in content):
                    complete_answer_chunks.append(chunk)
                    print(f"ğŸ¯ å®Œæ•´å›ç­”ç™ºè¦‹: chunk_{relevant_chunks.index(chunk)}")
            
            print(f"ğŸ“Š é–¢é€£ãƒãƒ£ãƒ³ã‚¯: {len(relevant_chunks)}å€‹")
            print(f"ğŸ“Š å®Œæ•´å›ç­”ãƒãƒ£ãƒ³ã‚¯: {len(complete_answer_chunks)}å€‹")
            
            if not complete_answer_chunks:
                print("âŒ å®Œæ•´å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            # 4. Weaviateã¸ã®ä¿å­˜ - å®Œæ•´å›ç­”ã‚’ä¼˜å…ˆ
            print("ğŸ’¾ Weaviateä¿å­˜é–‹å§‹...")
            
            # ã¾ãšå®Œæ•´å›ç­”ã‚’æœ€åˆã«ä¿å­˜ï¼ˆå„ªå…ˆåº¦å‘ä¸Šï¼‰
            priority_chunks = complete_answer_chunks + [
                chunk for chunk in relevant_chunks 
                if chunk not in complete_answer_chunks
            ]
            
            # ãƒ¡ã‚½ãƒƒãƒ‰å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not hasattr(self.weaviate_client, 'add_documents_with_external_vectors'):
                print("âŒ ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ä»£æ›¿ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨...")
                # ä»£æ›¿æ‰‹æ®µ: ç›´æ¥ add_documents ã‚’ä½¿ç”¨
                if hasattr(self.weaviate_client, 'add_documents'):
                    print("ğŸ”„ add_documents ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨...")
                    # å…ˆè·å–åµŒå…¥
                    texts = [chunk['content'] for chunk in priority_chunks]
                    embeddings = self.bedrock_service.get_embeddings(texts, input_type="search_document")
                    
                    # æ·»åŠ å‘é‡åˆ°æ–‡æ¡£
                    for i, chunk in enumerate(priority_chunks):
                        if i < len(embeddings):
                            chunk['vector'] = embeddings[i]
                    
                    success = self.weaviate_client.add_documents(priority_chunks)
                else:
                    print("âŒ ã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return False
            else:
                success = self.weaviate_client.add_documents_with_external_vectors(
                    priority_chunks, self.bedrock_service.get_embeddings
                )
            
            if success:
                print(f"âœ… ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº†: {len(priority_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯")
                self.knowledge_loaded = True
                return True
            else:
                print("âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ PDFãƒŠãƒ¬ãƒƒã‚¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def enhanced_search(self, query: str, top_k: int = 8) -> List[Dict]:
        """å¼·åŒ–ã•ã‚ŒãŸæ¤œç´¢ - ä¿®å¾©ç‰ˆ"""
        try:
            # æ­£ã—ã„æ¤œç´¢ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            if hasattr(self.weaviate_client, 'semantic_search_with_external_vector'):
                print(f"ğŸ” å¤–éƒ¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’ä½¿ç”¨: top_k={top_k}")
                basic_results = self.weaviate_client.semantic_search_with_external_vector(
                    query, self.bedrock_service.get_embeddings, top_k=top_k
                )
            elif hasattr(self.weaviate_client, 'semantic_search'):
                print(f"ğŸ” åŸºæœ¬æ¤œç´¢ã‚’ä½¿ç”¨: limit={top_k}")
                # semantic_searchã¯limitãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                basic_results = self.weaviate_client.semantic_search(query, limit=top_k)
            else:
                print("âŒ æ¤œç´¢ãƒ¡ã‚½ãƒƒãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
            
            print(f"ğŸ” åŸºæœ¬æ¤œç´¢çµæœ: {len(basic_results)}ä»¶")
            
            # å®Œæ•´å›ç­”ä¼˜å…ˆæ’åº
            def prioritize_complete_answers(results):
                complete_keywords = ['é›»æŸ±ç•ªå·', 'ä¸å…·åˆçŠ¶æ³', 'ç™ºç”Ÿæ™‚é–“å¸¯', 'ç™ºç”Ÿç¯„å›²']
                
                scored_results = []
                for result in results:
                    content = result.get('content', '')
                    
                    # å®Œæ•´æ€§è¯„åˆ†
                    completeness_score = sum(1 for kw in complete_keywords if kw in content)
                    
                    # å…³é”®çŸ­è¯­åŠ åˆ†
                    bonus_phrases = ['è¨˜å…¥ã®ãƒã‚¤ãƒ³ãƒˆ', 'é›»åœ§èª¿æŸ»ã«ã¤ã„ã¦', 'é›»åœ§ç•°å¸¸ã®å ´åˆ']
                    bonus_score = sum(2 for phrase in bonus_phrases if phrase in content)
                    
                    # è·å–ç›¸ä¼¼åº¦ï¼ˆå¤„ç†ä¸åŒçš„å­—æ®µåï¼‰
                    similarity = result.get('similarity', result.get('certainty', result.get('_additional', {}).get('certainty', 0)))
                    
                    # æ€»å¾—åˆ† = ç›¸ä¼¼åº¦ + å®Œæ•´æ€§åŠ åˆ† + å…³é”®çŸ­è¯­åŠ åˆ†
                    total_score = similarity + (completeness_score * 0.05) + (bonus_score * 0.03)
                    
                    result['total_score'] = total_score
                    result['completeness_score'] = completeness_score
                    result['similarity'] = similarity
                    scored_results.append(result)
                    
                    print(f"   - æ–‡æ›¸å¾—ç‚¹: ç›¸ä¼¼åº¦={similarity:.3f}, å®Œæ•´æ€§={completeness_score}/4, ç·åˆ={total_score:.3f}")
                
                return sorted(scored_results, key=lambda x: x['total_score'], reverse=True)
            
            prioritized_results = prioritize_complete_answers(basic_results)
            return prioritized_results[:3]
            
        except Exception as e:
            print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """å›ç­”ç”Ÿæˆ - å„ªåŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        
        if not context_docs:
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            content = doc.get('content', '')
            similarity = doc.get('similarity', 0)
            completeness = doc.get('completeness_score', 0)
            
            context_parts.append(f"""[æ¤œç´¢çµæœ{i}] (ç›¸ä¼¼åº¦: {similarity:.3f}, å®Œæ•´æ€§: {completeness}/4)
{content}""")
        
        context = "\n\n".join(context_parts)
        
        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        enhanced_prompt = f"""ã‚ãªãŸã¯é›»åŠ›è¨­å‚™ã®ç”³è¾¼ã¿æ¥­å‹™ã«ç²¾é€šã—ãŸã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆæ‹…å½“è€…ã§ã™ã€‚
æ¬¡ã«ç¤ºã™ã®ã¯ã€ç¤¾å†…ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‹ã‚‰æ¤œç´¢ã—ãŸé–¢é€£æ–‡æ›¸ã§ã™ã€‚

--- æ¤œç´¢çµæœé–‹å§‹ ---
{context}
--- æ¤œç´¢çµæœçµ‚äº† ---

è³ªå•: {query}

å›ç­”è¦ä»¶:
1. å›ç­”ã¯å¿…ãšæ¤œç´¢çµæœã«åŸºã¥ãã“ã¨
2. æ¤œç´¢çµæœã«æ˜ç¢ºãªè¨˜è¼‰ãŒãªã„å ´åˆã¯ã€Œãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”
3. å›ç­”ã¯æ—¥æœ¬èªã§ã€ç°¡æ½”ã‹ã¤æ­£ç¢ºã«è¨˜è¿°ã™ã‚‹ã“ã¨  
4. é›»åœ§èª¿æŸ»ã«é–¢ã™ã‚‹å…·ä½“çš„ãª4ã¤ã®æƒ…å ±ãŒè³ªå•ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§å›ç­”
5. å¿…è¦ã«å¿œã˜ã¦æ¤œç´¢çµæœã‹ã‚‰ã®å¼•ç”¨ï¼ˆæŠœç²‹ï¼‰ã‚’æ ¹æ‹ ã¨ã—ã¦ç¤ºã™ã“ã¨

å›ç­”:"""

        try:
            response = self.bedrock_service.generate_text_claude(enhanced_prompt)
            return response if response else "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            
        except Exception as e:
            print(f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """è³ªå•å¿œç­”ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        start_time = time.time()
        
        try:
            if not self.knowledge_loaded:
                return {
                    'answer': "ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                    'search_results': [],
                    'processing_time': 0,
                    'confidence': 0
                }
            
            # æ¤œç´¢å®Ÿè¡Œ
            print(f"ğŸ” æ¤œç´¢å®Ÿè¡Œ: '{question}'")
            search_results = self.enhanced_search(question, top_k=8)
            
            print(f"ğŸ“Š æ¤œç´¢çµæœ: {len(search_results)}ä»¶")
            for i, result in enumerate(search_results[:3], 1):
                completeness = result.get('completeness_score', 0)
                total_score = result.get('total_score', 0)
                print(f"   çµæœ{i}: ç›¸ä¼¼åº¦={result.get('similarity', 0):.3f}, å®Œæ•´æ€§={completeness}/4, ç·åˆ={total_score:.3f}")
            
            # å›ç­”ç”Ÿæˆ
            print("ğŸ¤– å›ç­”ç”Ÿæˆä¸­...")
            answer = self.generate_answer(question, search_results)
            
            # ä¿¡é ¼åº¦è¨ˆç®—
            confidence = 0
            if search_results:
                best_result = search_results[0]
                confidence = min(best_result.get('total_score', 0), 1.0)
            
            processing_time = time.time() - start_time
            
            return {
                'answer': answer,
                'search_results': search_results[:3],
                'processing_time': processing_time,
                'confidence': confidence
            }
            
        except Exception as e:
            print(f"âŒ è³ªå•å¿œç­”ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'answer': f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                'search_results': [],
                'processing_time': time.time() - start_time,
                'confidence': 0
            }
    
    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if hasattr(self.weaviate_client, 'close'):
                self.weaviate_client.close()
        except Exception as e:
            print(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {str(e)}")

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    print("ğŸ¯ æœ€çµ‚ç‰ˆRAGã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰")
    
    service = FinalWebRAGService()
    
    try:
        # PDFã‚’èª­ã¿è¾¼ã¿
        pdf_path = "../bedrock/pdf/high_takusoukun_web_manual_separate.pdf"
        if service.load_pdf_knowledge(pdf_path):
            
            # ãƒ†ã‚¹ãƒˆè³ªå•
            test_questions = [
                "é›»åœ§èª¿æŸ»ã§ã¯ã€ã©ã®4ã¤ã®æƒ…å ±ã‚’å„ªå…ˆçš„ã«åé›†ã™ã¹ãã§ã™ã‹ï¼Ÿ",
                "é›»åœ§èª¿æŸ»ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
                "é›»åœ§ç•°å¸¸èª¿æŸ»ã§ã®è¨˜å…¥ãƒã‚¤ãƒ³ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ"
            ]
            
            for question in test_questions:
                print(f"\n{'='*50}")
                print(f"è³ªå•: {question}")
                print('='*50)
                
                result = service.ask_question(question)
                print(f"\nå›ç­”:\n{result['answer']}")
                print(f"\nä¿¡é ¼åº¦: {result['confidence']:.3f}")
                print(f"å‡¦ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
        else:
            print("âŒ PDFãƒŠãƒ¬ãƒƒã‚¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    finally:
        # ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        service.close() 