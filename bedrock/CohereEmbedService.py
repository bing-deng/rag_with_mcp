#!/usr/bin/env python3
"""
Cohere Embed Multilingual ä¸“ç”¨æœåŠ¡ç±»
åŸºäºå®é™…æµ‹è¯•ç»“æœä¼˜åŒ–çš„å®ç°
"""

import boto3
import json
import numpy as np
from botocore.exceptions import ClientError
from typing import List, Dict, Any, Optional, Union
import time

class CohereEmbedService:
    """Cohere Embed Multilingual ä¸“ç”¨æœåŠ¡ç±»"""
    
    def __init__(self, region_name: str = 'ap-northeast-1'):
        """
        åˆå§‹åŒ–CohereåµŒå…¥æœåŠ¡
        
        Args:
            region_name: AWSåŒºåŸŸåç§°
        """
        self.region_name = region_name
        self.bedrock_runtime = boto3.client('bedrock-runtime', region_name=region_name)
        
        # Cohereæ¨¡å‹é…ç½®
        self.models = {
            'multilingual': 'cohere.embed-multilingual-v3',
            'english': 'cohere.embed-english-v3'
        }
        
        self.default_model = self.models['multilingual']
        
        # åŸºäºæµ‹è¯•ç»“æœçš„é…ç½®
        self.vector_dimension = 1024
        self.max_batch_size = 50  # æµ‹è¯•æ˜¾ç¤ºå¯ä»¥å¤„ç†50ä¸ªæ–‡æœ¬
        
    def _make_request(
        self, 
        texts: List[str], 
        input_type: str = "search_document",
        model: str = None,
        embedding_types: List[str] = None
    ) -> Optional[Dict]:
        """
        å‘é€è¯·æ±‚åˆ°Cohereæ¨¡å‹
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            input_type: è¾“å…¥ç±»å‹
            model: æ¨¡å‹åç§°
            embedding_types: åµŒå…¥ç±»å‹
            
        Returns:
            APIå“åº”ç»“æœæˆ–None
        """
        if model is None:
            model = self.default_model
        elif model in self.models:
            model = self.models[model]
            
        if embedding_types is None:
            embedding_types = ["float"]
            
        try:
            body = {
                "texts": texts,
                "input_type": input_type,
                "embedding_types": embedding_types,
                "truncate": "END"
            }
            
            response = self.bedrock_runtime.invoke_model(
                modelId=model,
                contentType='application/json',
                accept='*/*',
                body=json.dumps(body)
            )
            
            result = json.loads(response['body'].read())
            return result
            
        except ClientError as e:
            print(f"Cohere APIè°ƒç”¨å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"å…¶ä»–é”™è¯¯: {e}")
            return None
    
    def get_embeddings(
        self, 
        texts: Union[str, List[str]], 
        input_type: str = "search_document",
        model: str = None,
        batch_processing: bool = False
    ) -> List[List[float]]:
        """
        è·å–æ–‡æœ¬åµŒå…¥å‘é‡
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            input_type: è¾“å…¥ç±»å‹ (search_document, search_query, classification, clustering)
            model: æ¨¡å‹åç§° ('multilingual', 'english' æˆ–å®Œæ•´æ¨¡å‹ID)
            batch_processing: æ˜¯å¦å¼ºåˆ¶æ‰¹é‡å¤„ç†
            
        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        # æ ‡å‡†åŒ–è¾“å…¥
        if isinstance(texts, str):
            texts = [texts]
        
        if not texts:
            return []
        
        # åŸºäºæµ‹è¯•ç»“æœï¼šçœ‹èµ·æ¥Cohereæ¨¡å‹å¯èƒ½éœ€è¦é€ä¸ªå¤„ç†
        # æˆ–è€…å“åº”æ ¼å¼ä¸é¢„æœŸä¸åŒ
        embeddings = []
        
        if batch_processing and len(texts) <= self.max_batch_size:
            # å°è¯•æ‰¹é‡å¤„ç†
            result = self._make_request(texts, input_type, model)
            if result and 'embeddings' in result:
                embeddings = result['embeddings']
                
                # æ£€æŸ¥è¿”å›çš„å‘é‡æ•°é‡
                if len(embeddings) != len(texts):
                    print(f"âš ï¸ æ‰¹é‡å¤„ç†å¼‚å¸¸: æœŸæœ›{len(texts)}ä¸ªå‘é‡ï¼Œå®é™…{len(embeddings)}ä¸ª")
                    # å¦‚æœæ‰¹é‡å¤„ç†æœ‰é—®é¢˜ï¼Œå›é€€åˆ°é€ä¸ªå¤„ç†
                    return self._process_individually(texts, input_type, model)
                    
                return embeddings
            else:
                # æ‰¹é‡å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªå¤„ç†
                return self._process_individually(texts, input_type, model)
        else:
            # é€ä¸ªå¤„ç†ï¼ˆåŸºäºæµ‹è¯•ç»“æœï¼Œè¿™å¯èƒ½æ˜¯æ›´å¯é çš„æ–¹å¼ï¼‰
            return self._process_individually(texts, input_type, model)
    
    def _process_individually(
        self, 
        texts: List[str], 
        input_type: str,
        model: str
    ) -> List[List[float]]:
        """
        é€ä¸ªå¤„ç†æ–‡æœ¬ï¼ˆåŸºäºæµ‹è¯•ç»“æœçš„å¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            input_type: è¾“å…¥ç±»å‹
            model: æ¨¡å‹åç§°
            
        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        embeddings = []
        
        for i, text in enumerate(texts):
            result = self._make_request([text], input_type, model)
            
            if result and 'embeddings' in result:
                embedding_list = result['embeddings']
                
                # æ£€æŸ¥embeddingsæ˜¯å¦ä¸ºåˆ—è¡¨ä¸”ä¸ä¸ºç©º
                if isinstance(embedding_list, list) and len(embedding_list) > 0:
                    # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºNone
                    if embedding_list[0] is not None:
                        embeddings.append(embedding_list[0])
                    else:
                        print(f"âš ï¸ ç¬¬{i+1}ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ä¸ºç©º: {text[:30]}...")
                        embeddings.append(None)
                else:
                    print(f"âš ï¸ ç¬¬{i+1}ä¸ªæ–‡æœ¬è¿”å›çš„embeddingsæ ¼å¼å¼‚å¸¸: {text[:30]}...")
                    print(f"    embeddingså†…å®¹: {embedding_list}")
                    embeddings.append(None)
            else:
                print(f"âš ï¸ ç¬¬{i+1}ä¸ªæ–‡æœ¬è¯·æ±‚å¤±è´¥: {text[:30]}...")
                if result:
                    print(f"    å“åº”å†…å®¹: {result}")
                embeddings.append(None)
            
            # æ·»åŠ å°å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
            if i < len(texts) - 1:
                time.sleep(0.1)
        
        return embeddings
    
    def semantic_search(
        self, 
        query: str, 
        documents: List[str], 
        top_k: int = 5,
        model: str = None
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            model: æ¨¡å‹åç§°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not query or not documents:
            return []
        
        # è·å–æŸ¥è¯¢åµŒå…¥å‘é‡
        query_embeddings = self.get_embeddings(
            texts=[query],
            input_type="search_query",
            model=model
        )
        
        if not query_embeddings or query_embeddings[0] is None:
            print("âŒ æŸ¥è¯¢åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥")
            return []
        
        # è·å–æ–‡æ¡£åµŒå…¥å‘é‡
        doc_embeddings = self.get_embeddings(
            texts=documents,
            input_type="search_document",
            model=model
        )
        
        if not doc_embeddings:
            print("âŒ æ–‡æ¡£åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥")
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        query_vec = np.array(query_embeddings[0])
        similarities = []
        
        for i, doc_embedding in enumerate(doc_embeddings):
            if doc_embedding is not None:
                doc_vec = np.array(doc_embedding)
                
                # ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(query_vec, doc_vec) / (
                    np.linalg.norm(query_vec) * np.linalg.norm(doc_vec)
                )
                
                similarities.append({
                    'index': i,
                    'document': documents[i],
                    'similarity': float(similarity)
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:top_k]
    
    def cross_lingual_search(
        self, 
        query: str, 
        multilingual_documents: List[Dict[str, str]], 
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        è·¨è¯­è¨€æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆä»»æ„è¯­è¨€ï¼‰
            multilingual_documents: å¤šè¯­è¨€æ–‡æ¡£åˆ—è¡¨ [{"text": "å†…å®¹", "language": "zh", "metadata": {...}}]
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            
        Returns:
            è·¨è¯­è¨€æœç´¢ç»“æœ
        """
        documents = [doc["text"] for doc in multilingual_documents]
        
        # ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹è¿›è¡Œæœç´¢
        search_results = self.semantic_search(
            query=query,
            documents=documents,
            top_k=top_k,
            model='multilingual'
        )
        
        # æ·»åŠ è¯­è¨€å’Œå…ƒæ•°æ®ä¿¡æ¯
        for result in search_results:
            original_doc = multilingual_documents[result['index']]
            result['language'] = original_doc.get('language', 'unknown')
            result['metadata'] = original_doc.get('metadata', {})
        
        return search_results
    
    def classify_texts(
        self, 
        texts: List[str], 
        categories: List[str],
        model: str = None
    ) -> List[Dict[str, Any]]:
        """
        æ–‡æœ¬åˆ†ç±»
        
        Args:
            texts: å¾…åˆ†ç±»æ–‡æœ¬åˆ—è¡¨
            categories: ç±»åˆ«åˆ—è¡¨
            model: æ¨¡å‹åç§°
            
        Returns:
            åˆ†ç±»ç»“æœåˆ—è¡¨
        """
        if not texts or not categories:
            return []
        
        # è·å–ç±»åˆ«åµŒå…¥å‘é‡
        category_embeddings = self.get_embeddings(
            texts=categories,
            input_type="classification",
            model=model
        )
        
        # è·å–æ–‡æœ¬åµŒå…¥å‘é‡
        text_embeddings = self.get_embeddings(
            texts=texts,
            input_type="classification",
            model=model
        )
        
        if not category_embeddings or not text_embeddings:
            return []
        
        results = []
        
        for i, text_embedding in enumerate(text_embeddings):
            if text_embedding is None:
                results.append({
                    'text': texts[i],
                    'predicted_category': None,
                    'confidence': 0.0,
                    'all_scores': {}
                })
                continue
            
            text_vec = np.array(text_embedding)
            scores = {}
            best_category = None
            best_score = -1
            
            for j, category_embedding in enumerate(category_embeddings):
                if category_embedding is not None:
                    category_vec = np.array(category_embedding)
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(text_vec, category_vec) / (
                        np.linalg.norm(text_vec) * np.linalg.norm(category_vec)
                    )
                    
                    scores[categories[j]] = float(similarity)
                    
                    if similarity > best_score:
                        best_score = similarity
                        best_category = categories[j]
            
            results.append({
                'text': texts[i],
                'predicted_category': best_category,
                'confidence': float(best_score),
                'all_scores': scores
            })
        
        return results
    
    def compute_similarity_matrix(
        self, 
        texts: List[str],
        input_type: str = "search_document",
        model: str = None
    ) -> np.ndarray:
        """
        è®¡ç®—æ–‡æœ¬é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            input_type: è¾“å…¥ç±»å‹
            model: æ¨¡å‹åç§°
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ (n x n)
        """
        if not texts:
            return np.array([])
        
        embeddings = self.get_embeddings(
            texts=texts,
            input_type=input_type,
            model=model
        )
        
        if not embeddings or any(emb is None for emb in embeddings):
            print("âŒ éƒ¨åˆ†åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥")
            return np.array([])
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        vectors = np.array(embeddings)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # å…ˆå½’ä¸€åŒ–
        normalized_vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = np.dot(normalized_vectors, normalized_vectors.T)
        
        return similarity_matrix
    
    def find_duplicates(
        self, 
        texts: List[str], 
        threshold: float = 0.95,
        model: str = None
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾é‡å¤æˆ–è¿‘ä¼¼é‡å¤çš„æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            model: æ¨¡å‹åç§°
            
        Returns:
            é‡å¤æ–‡æœ¬ç»„åˆ—è¡¨
        """
        similarity_matrix = self.compute_similarity_matrix(texts, model=model)
        
        if similarity_matrix.size == 0:
            return []
        
        duplicates = []
        processed = set()
        
        for i in range(len(texts)):
            if i in processed:
                continue
            
            # æ‰¾åˆ°ä¸å½“å‰æ–‡æœ¬ç›¸ä¼¼çš„æ‰€æœ‰æ–‡æœ¬
            similar_indices = []
            for j in range(i, len(texts)):
                if similarity_matrix[i, j] >= threshold:
                    similar_indices.append(j)
                    processed.add(j)
            
            if len(similar_indices) > 1:
                duplicates.append({
                    'group_id': len(duplicates),
                    'similar_texts': [
                        {
                            'index': idx,
                            'text': texts[idx],
                            'similarity_to_first': float(similarity_matrix[i, idx])
                        }
                        for idx in similar_indices
                    ],
                    'count': len(similar_indices)
                })
        
        return duplicates
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return {
            'available_models': self.models,
            'default_model': self.default_model,
            'vector_dimension': self.vector_dimension,
            'max_batch_size': self.max_batch_size,
            'supported_languages': '100+',
            'input_types': [
                'search_document',
                'search_query', 
                'classification',
                'clustering'
            ],
            'features': [
                'multilingual_support',
                'cross_lingual_search',
                'text_classification',
                'semantic_similarity',
                'duplicate_detection'
            ]
        }

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_cohere_service():
    """æµ‹è¯•CohereæœåŠ¡çš„å„é¡¹åŠŸèƒ½"""
    
    print("=== Cohere Embed Service åŠŸèƒ½æµ‹è¯• ===")
    
    # åˆå§‹åŒ–æœåŠ¡
    cohere = CohereEmbedService(region_name='ap-northeast-1')
    
    # 1. åŸºæœ¬åµŒå…¥æµ‹è¯•
    print("\n1. åŸºæœ¬åµŒå…¥å‘é‡æµ‹è¯•...")
    texts = ["äººå·¥æ™ºèƒ½", "Artificial Intelligence", "äººå·¥çŸ¥èƒ½"]
    embeddings = cohere.get_embeddings(texts)
    
    successful_embeddings = [emb for emb in embeddings if emb is not None]
    print(f"   ğŸ“Š æˆåŠŸç”Ÿæˆ {len(successful_embeddings)}/{len(texts)} ä¸ªåµŒå…¥å‘é‡")
    
    if successful_embeddings:
        print(f"   ğŸ“ å‘é‡ç»´åº¦: {len(successful_embeddings[0])}")
    
    # 2. è¯­ä¹‰æœç´¢æµ‹è¯•
    print("\n2. è¯­ä¹‰æœç´¢æµ‹è¯•...")
    query = "æœºå™¨å­¦ä¹ ç®—æ³•"
    documents = [
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "Machine learning algorithms are powerful",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å‘å±•è¿…é€Ÿ",
        "I like to eat pizza"
    ]
    
    search_results = cohere.semantic_search(query, documents, top_k=3)
    
    if search_results:
        print(f"   âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
        for i, result in enumerate(search_results, 1):
            print(f"      {i}. {result['document'][:50]}... (ç›¸ä¼¼åº¦: {result['similarity']:.4f})")
    else:
        print("   âŒ æœç´¢å¤±è´¥")
    
    # 3. è·¨è¯­è¨€æœç´¢æµ‹è¯•
    print("\n3. è·¨è¯­è¨€æœç´¢æµ‹è¯•...")
    multilingual_docs = [
        {"text": "äººå·¥æ™ºèƒ½æ”¹å˜äº†ä¸–ç•Œ", "language": "zh"},
        {"text": "AI is transforming the world", "language": "en"},
        {"text": "AIãŒä¸–ç•Œã‚’å¤‰ãˆã¦ã„ã‚‹", "language": "ja"},
        {"text": "ä»Šå¤©åƒä»€ä¹ˆå¥½å‘¢", "language": "zh"}
    ]
    
    cross_results = cohere.cross_lingual_search(
        query="What is artificial intelligence?",
        multilingual_documents=multilingual_docs,
        top_k=3
    )
    
    if cross_results:
        print(f"   âœ… è·¨è¯­è¨€æœç´¢æˆåŠŸ")
        for result in cross_results:
            print(f"      [{result['language']}] {result['document']} (ç›¸ä¼¼åº¦: {result['similarity']:.4f})")
    else:
        print("   âŒ è·¨è¯­è¨€æœç´¢å¤±è´¥")
    
    # 4. æ–‡æœ¬åˆ†ç±»æµ‹è¯•
    print("\n4. æ–‡æœ¬åˆ†ç±»æµ‹è¯•...")
    classification_texts = [
        "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„",
        "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œä¸æ¨è",
        "ä»·æ ¼è¿˜å¯ä»¥æ¥å—"
    ]
    categories = ["æ­£é¢è¯„ä»·", "è´Ÿé¢è¯„ä»·", "ä¸­æ€§è¯„ä»·"]
    
    classification_results = cohere.classify_texts(classification_texts, categories)
    
    if classification_results:
        print("   âœ… åˆ†ç±»æˆåŠŸ")
        for result in classification_results:
            if result['predicted_category']:
                print(f"      æ–‡æœ¬: {result['text']}")
                print(f"      åˆ†ç±»: {result['predicted_category']} (ç½®ä¿¡åº¦: {result['confidence']:.4f})")
    else:
        print("   âŒ åˆ†ç±»å¤±è´¥")
    
    # 5. æ¨¡å‹ä¿¡æ¯
    print("\n5. æ¨¡å‹ä¿¡æ¯...")
    model_info = cohere.get_model_info()
    print(f"   ğŸ“‹ å¯ç”¨æ¨¡å‹: {list(model_info['available_models'].keys())}")
    print(f"   ğŸŒ æ”¯æŒè¯­è¨€: {model_info['supported_languages']}")
    print(f"   ğŸ“ å‘é‡ç»´åº¦: {model_info['vector_dimension']}")

if __name__ == "__main__":
    test_cohere_service()