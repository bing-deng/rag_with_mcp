#!/usr/bin/env python3
"""
HTML å†…å®¹å‘é‡åŒ–å­˜å‚¨ç³»ç»Ÿ
å°† HTML ç½‘é¡µå†…å®¹è§£æã€æ¸…ç†ã€å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° Milvus å‘é‡æ•°æ®åº“ä¸­
"""

import os
import re
import time
import hashlib
import requests
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Tuple, Optional
import numpy as np

# HTML è§£æç›¸å…³
from bs4 import BeautifulSoup

# Milvus ç›¸å…³
from pymilvus import (
    connections, utility, FieldSchema, CollectionSchema, 
    DataType, Collection, MilvusException
)

# å‘é‡åŒ–ç›¸å…³ (å¦‚æœæ²¡æœ‰å®‰è£…ä¼šæä¾›æ›¿ä»£æ–¹æ¡ˆ)
try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
    print("âœ… ä½¿ç”¨ sentence-transformers è¿›è¡Œè¯­ä¹‰å‘é‡åŒ–")
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("âš ï¸  sentence-transformers æœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–å‘é‡åŒ–")
    print("   å®‰è£…å‘½ä»¤: pip install sentence-transformers beautifulsoup4 requests")

class HTMLToMilvusProcessor:
    """HTML å†…å®¹åˆ° Milvus çš„å¤„ç†å™¨"""
    
    def __init__(self, host='localhost', port='19530', collection_name='html_content'):
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.collection = None
        self.dimension = 384
        
        # åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹
        if HAS_SENTENCE_TRANSFORMERS:
            print("ğŸ”§ åŠ è½½è¯­ä¹‰å‘é‡åŒ–æ¨¡å‹...")
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… è¯­ä¹‰æ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            self.model = None
    
    def connect_to_milvus(self):
        """è¿æ¥åˆ° Milvus"""
        try:
            connections.connect("default", host=self.host, port=self.port)
            print(f"âœ… æˆåŠŸè¿æ¥åˆ° Milvus: {self.host}:{self.port}")
            return True
        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
            return False
    
    def create_collection(self):
        """åˆ›å»º HTML å†…å®¹é›†åˆ"""
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=15000),
            FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=50),  # paragraph, heading, listç­‰
            FieldSchema(name="word_count", dtype=DataType.INT64),
            FieldSchema(name="url_hash", dtype=DataType.VARCHAR, max_length=64),
            FieldSchema(name="timestamp", dtype=DataType.INT64),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dimension)
        ]
        
        schema = CollectionSchema(fields, "HTML å†…å®¹å‘é‡åŒ–å­˜å‚¨é›†åˆ")
        
        if utility.has_collection(self.collection_name):
            print(f"ğŸ“ é›†åˆ {self.collection_name} å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤...")
            utility.drop_collection(self.collection_name)
        
        self.collection = Collection(self.collection_name, schema)
        print(f"âœ… é›†åˆ {self.collection_name} åˆ›å»ºæˆåŠŸ")
        return self.collection
    
    def create_index(self):
        """åˆ›å»ºå‘é‡ç´¢å¼•"""
        index_params = {
            "metric_type": "COSINE",
            "index_type": "HNSW",
            "params": {"M": 16, "efConstruction": 200}
        }
        
        print("ğŸ”§ æ­£åœ¨åˆ›å»ºç´¢å¼•...")
        self.collection.create_index("embedding", index_params)
        print("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")
    
    def load_collection(self):
        """åŠ è½½é›†åˆåˆ°å†…å­˜"""
        print("ğŸ“¥ æ­£åœ¨åŠ è½½é›†åˆåˆ°å†…å­˜...")
        self.collection.load()
        print("âœ… é›†åˆåŠ è½½å®Œæˆ")
    
    def fetch_html(self, url: str) -> Optional[str]:
        """è·å– HTML å†…å®¹"""
        try:
            print(f"ğŸŒ æ­£åœ¨è·å–ç½‘é¡µ: {url}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            print(f"âœ… æˆåŠŸè·å–ç½‘é¡µå†…å®¹ ({len(response.text)} å­—ç¬¦)")
            return response.text
        except Exception as e:
            print(f"âŒ è·å–ç½‘é¡µå¤±è´¥: {e}")
            return None
    
    def parse_html(self, html_content: str, base_url: str = "") -> List[Dict]:
        """è§£æ HTML å†…å®¹å¹¶æå–ç»“æ„åŒ–ä¿¡æ¯"""
        print("ğŸ” æ­£åœ¨è§£æ HTML å†…å®¹...")
        
        soup = BeautifulSoup(html_content, 'html.parser', from_encoding='utf-8')
        
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
            tag.decompose()
        
        # æå–ç½‘é¡µæ ‡é¢˜
        title = soup.find('title')
        page_title = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        extracted_content = []
        
        # 1. æå–æ ‡é¢˜å†…å®¹ (h1-h6)
        for level in range(1, 7):
            headings = soup.find_all(f'h{level}')
            for heading in headings:
                text = self._clean_text(heading.get_text())
                if text and len(text) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                    extracted_content.append({
                        'content': text,
                        'content_type': f'heading_h{level}',
                        'word_count': len(text.split())
                    })
        
        # 2. æå–æ®µè½å†…å®¹
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = self._clean_text(p.get_text())
            if text and len(text) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                # å¦‚æœæ®µè½å¤ªé•¿ï¼Œåˆ†å‰²æˆå¤šä¸ªå—
                chunks = self._split_long_text(text, max_length=1000)
                for i, chunk in enumerate(chunks):
                    content_type = 'paragraph' if len(chunks) == 1 else f'paragraph_chunk_{i+1}'
                    extracted_content.append({
                        'content': chunk,
                        'content_type': content_type,
                        'word_count': len(chunk.split())
                    })
        
        # 3. æå–åˆ—è¡¨å†…å®¹
        lists = soup.find_all(['ul', 'ol'])
        for lst in lists:
            items = lst.find_all('li')
            if len(items) > 1:  # è‡³å°‘æœ‰2ä¸ªåˆ—è¡¨é¡¹æ‰å¤„ç†
                list_text = []
                for item in items:
                    item_text = self._clean_text(item.get_text())
                    if item_text:
                        list_text.append(f"â€¢ {item_text}")
                
                if list_text:
                    combined_text = '\n'.join(list_text[:10])  # æœ€å¤šå–å‰10é¡¹
                    extracted_content.append({
                        'content': combined_text,
                        'content_type': 'list',
                        'word_count': len(combined_text.split())
                    })
        
        # 4. æå–è¡¨æ ¼å†…å®¹ (ç®€åŒ–å¤„ç†)
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            if len(rows) > 1:  # è‡³å°‘æœ‰è¡¨å¤´å’Œä¸€è¡Œæ•°æ®
                table_text = []
                for row in rows[:5]:  # æœ€å¤šå–å‰5è¡Œ
                    cells = row.find_all(['td', 'th'])
                    row_text = ' | '.join([self._clean_text(cell.get_text()) for cell in cells])
                    if row_text.strip():
                        table_text.append(row_text)
                
                if table_text:
                    combined_text = '\n'.join(table_text)
                    extracted_content.append({
                        'content': combined_text,
                        'content_type': 'table',
                        'word_count': len(combined_text.split())
                    })
        
        # 5. æå–å®šä¹‰åˆ—è¡¨å†…å®¹ (dt/dd) - é‡è¦çš„ä¼ä¸šä¿¡æ¯é€šå¸¸åœ¨è¿™é‡Œ
        dts = soup.find_all('dt')
        dds = soup.find_all('dd')
        if dts and dds:
            definition_pairs = []
            for i, dt in enumerate(dts):
                dt_text = self._clean_text(dt.get_text())
                if i < len(dds):
                    dd = dds[i]
                    dd_text = self._clean_text(dd.get_text())
                    if dt_text and dd_text:
                        definition_pairs.append(f"{dt_text}: {dd_text}")
            
            if definition_pairs:
                # å°†å®šä¹‰åˆ—è¡¨ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„å†…å®¹å— 
                combined_text = '\n'.join(definition_pairs)
                extracted_content.append({
                    'content': combined_text,
                    'content_type': 'definition_list',
                    'word_count': len(combined_text.split())
                })
                
                # å¯¹äºé‡è¦çš„å®šä¹‰é¡¹ï¼Œä¹Ÿå•ç‹¬åˆ›å»ºå†…å®¹å—ï¼ˆä¾¿äºç²¾ç¡®æœç´¢ï¼‰
                for pair in definition_pairs:
                    if any(keyword in pair for keyword in ['è¨­ç«‹', 'å‰µç«‹', 'è³‡æœ¬é‡‘', 'å¾“æ¥­å“¡', 'å£²ä¸Š', 'æœ¬ç¤¾', 'ä¼šç¤¾å']):
                        extracted_content.append({
                            'content': pair,
                            'content_type': 'company_info',
                            'word_count': len(pair.split())
                        })
        
        # 6. æå–åœ°å€ä¿¡æ¯ (addressæ ‡ç­¾)
        addresses = soup.find_all('address')
        for addr in addresses:
            addr_text = self._clean_text(addr.get_text())
            if addr_text and len(addr_text) > 10:
                extracted_content.append({
                    'content': addr_text,
                    'content_type': 'address',
                    'word_count': len(addr_text.split())
                })
        
        # 7. æå–å¼•ç”¨å†…å®¹ (blockquoteæ ‡ç­¾)
        blockquotes = soup.find_all('blockquote')
        for quote in blockquotes:
            quote_text = self._clean_text(quote.get_text())
            if quote_text and len(quote_text) > 20:
                extracted_content.append({
                    'content': quote_text,
                    'content_type': 'quote',
                    'word_count': len(quote_text.split())
                })
        
        # 8. æå–å¼ºè°ƒå†…å®¹ (strong, em, bæ ‡ç­¾ä¸­çš„é‡è¦ä¿¡æ¯)
        emphasis_tags = soup.find_all(['strong', 'em', 'b'])
        emphasis_texts = []
        for tag in emphasis_tags:
            text = self._clean_text(tag.get_text())
            if text and len(text) > 3 and len(text) < 100:  # é¿å…å¤ªé•¿çš„å†…å®¹
                emphasis_texts.append(text)
        
        if emphasis_texts:
            # åˆå¹¶å¼ºè°ƒå†…å®¹
            combined_emphasis = ' | '.join(list(set(emphasis_texts))[:10])  # å»é‡å¹¶é™åˆ¶æ•°é‡
            extracted_content.append({
                'content': combined_emphasis,
                'content_type': 'emphasis',
                'word_count': len(combined_emphasis.split())
            })
        
        # 9. æå–è¯­ä¹‰åŒ–å†…å®¹ (section, articleæ ‡ç­¾)
        semantic_tags = soup.find_all(['section', 'article'])
        for tag in semantic_tags:
            # è·å–section/articleçš„ç›´æ¥æ–‡æœ¬å†…å®¹ï¼ˆæ’é™¤åµŒå¥—çš„å…¶ä»–æ ‡ç­¾ï¼‰
            tag_text = ""
            for string in tag.stripped_strings:
                if len(tag_text + string) < 500:  # é™åˆ¶é•¿åº¦
                    tag_text += string + " "
                else:
                    break
            
            tag_text = self._clean_text(tag_text)
            if tag_text and len(tag_text) > 30:
                extracted_content.append({
                    'content': tag_text,
                    'content_type': f'semantic_{tag.name}',
                    'word_count': len(tag_text.split())
                })
        
        # 10. æå–é¢„æ ¼å¼åŒ–å†…å®¹ (pre, codeæ ‡ç­¾)
        pre_tags = soup.find_all(['pre', 'code'])
        for tag in pre_tags:
            pre_text = tag.get_text()
            if pre_text and len(pre_text.strip()) > 10:
                # ä¿æŒåŸå§‹æ ¼å¼
                extracted_content.append({
                    'content': pre_text.strip(),
                    'content_type': f'code_{tag.name}',
                    'word_count': len(pre_text.split())
                })
        
        # 11. æå–å›¾è¡¨æ ‡é¢˜ (figcaptionæ ‡ç­¾)
        figcaptions = soup.find_all('figcaption')
        for cap in figcaptions:
            cap_text = self._clean_text(cap.get_text())
            if cap_text and len(cap_text) > 5:
                extracted_content.append({
                    'content': cap_text,
                    'content_type': 'figure_caption',
                    'word_count': len(cap_text.split())
                })
        
        # 12. æå–é‡è¦çš„divå†…å®¹ï¼ˆåŸºäºclassæˆ–idåˆ¤æ–­é‡è¦æ€§ï¼‰
        important_divs = soup.find_all('div', class_=re.compile(r'(info|profile|about|company|business|service|feature|highlight|summary|overview|description)', re.I))
        important_divs += soup.find_all('div', id=re.compile(r'(info|profile|about|company|business|service|feature|highlight|summary|overview|description)', re.I))
        
        for div in important_divs[:5]:  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
            # æå–divçš„ç›´æ¥æ–‡æœ¬å†…å®¹
            div_text = ""
            for string in div.stripped_strings:
                if len(div_text + string) < 800:
                    div_text += string + " "
                else:
                    break
            
            div_text = self._clean_text(div_text)
            if div_text and len(div_text) > 30:
                # å°è¯•ä»classæˆ–idè·å–æ›´å…·ä½“çš„ç±»å‹
                class_names = div.get('class', [])
                div_id = div.get('id', '')
                content_type = 'important_div'
                
                if any('company' in cls.lower() for cls in class_names) or 'company' in div_id.lower():
                    content_type = 'company_div'
                elif any('profile' in cls.lower() for cls in class_names) or 'profile' in div_id.lower():
                    content_type = 'profile_div'
                elif any('business' in cls.lower() for cls in class_names) or 'business' in div_id.lower():
                    content_type = 'business_div'
                
                extracted_content.append({
                    'content': div_text,
                    'content_type': content_type,
                    'word_count': len(div_text.split())
                })
        
        # 13. æå–spanä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆæ•°å­—ã€æ—¥æœŸã€é‡è¦æ ‡è¯†ï¼‰
        spans = soup.find_all('span')
        important_spans = []
        for span in spans:
            span_text = self._clean_text(span.get_text())
            if span_text and len(span_text) > 2 and len(span_text) < 50:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦ä¿¡æ¯ï¼ˆæ•°å­—ã€æ—¥æœŸã€è´§å¸ç­‰ï¼‰
                if re.search(r'(\d{4}å¹´|\d+å„„|\d+ä¸‡|\d+å††|\d+å|\d+%)', span_text):
                    important_spans.append(span_text)
        
        if important_spans:
            # åˆå¹¶é‡è¦çš„spanå†…å®¹
            combined_spans = ' | '.join(list(set(important_spans))[:8])
            extracted_content.append({
                'content': combined_spans,
                'content_type': 'key_data',
                'word_count': len(combined_spans.split())
            })
        
        # ä¸ºæ¯ä¸ªå†…å®¹å—æ·»åŠ å…¬å…±ä¿¡æ¯
        url_hash = hashlib.md5(base_url.encode()).hexdigest()
        timestamp = int(time.time())
        
        for content in extracted_content:
            content.update({
                'url': base_url,
                'title': page_title,
                'url_hash': url_hash,
                'timestamp': timestamp
            })
        
        print(f"âœ… è§£æå®Œæˆï¼Œæå–äº† {len(extracted_content)} ä¸ªå†…å®¹å—")
        return extracted_content
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€æ—¥æ–‡å’ŒåŸºæœ¬æ ‡ç‚¹
        # \u4e00-\u9fff: ä¸­æ—¥éŸ©æ±‰å­—
        # \u3040-\u309f: å¹³å‡å
        # \u30a0-\u30ff: ç‰‡å‡å
        # \u3000-\u303f: æ—¥æ–‡æ ‡ç‚¹ç¬¦å·
        # \uff00-\uffef: å…¨è§’å­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\u3000-\u303f\uff00-\uffef.,!?;:()"\'-]', '', text)
        
        return text
    
    def _split_long_text(self, text: str, max_length: int = 1000) -> List[str]:
        """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒçŸ­çš„å—"""
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) < max_length:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [text[:max_length]]
    
    def text_to_vector(self, text: str) -> List[float]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        if HAS_SENTENCE_TRANSFORMERS and self.model:
            embedding = self.model.encode(text, normalize_embeddings=True)
            return embedding.tolist()
        else:
            # ç®€åŒ–å‘é‡åŒ–æ–¹æ³•
            words = text.lower().split()
            vector = np.random.normal(0, 1, self.dimension)
            
            # åŸºäºæ–‡æœ¬é•¿åº¦å’Œå†…å®¹è°ƒæ•´å‘é‡
            length_factor = min(len(words) / 100.0, 1.0)
            vector = vector * length_factor
            
            # å½’ä¸€åŒ–
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
            
            return vector.tolist()
    
    def insert_html_content(self, content_blocks: List[Dict]) -> bool:
        """å°†è§£æåçš„ HTML å†…å®¹æ’å…¥ Milvus"""
        if not content_blocks:
            print("âš ï¸  æ²¡æœ‰å†…å®¹éœ€è¦æ’å…¥")
            return False
        
        print(f"ğŸ“ æ­£åœ¨å‘é‡åŒ–å¹¶æ’å…¥ {len(content_blocks)} ä¸ªå†…å®¹å—...")
        
        # å‡†å¤‡æ•°æ®
        urls = []
        titles = []
        contents = []
        content_types = []
        word_counts = []
        url_hashes = []
        timestamps = []
        embeddings = []
        
        for i, block in enumerate(content_blocks):
            print(f"  ğŸ“Š å‘é‡åŒ–å†…å®¹å— {i+1}/{len(content_blocks)}: {block['content_type']}")
            
            # å¤„ç†å†…å®¹é•¿åº¦é™åˆ¶
            content = block['content']
            if len(content) > 15000:
                print(f"  âš ï¸  å†…å®¹è¿‡é•¿ ({len(content)} å­—ç¬¦)ï¼Œæˆªæ–­åˆ° 15000 å­—ç¬¦")
                content = content[:14900] + "..."  # ç•™ä¸€äº›ä½™é‡
            
            # å¤„ç†æ ‡é¢˜é•¿åº¦é™åˆ¶
            title = block['title']
            if len(title) > 500:
                title = title[:497] + "..."
            
            # å¤„ç†URLé•¿åº¦é™åˆ¶
            url = block['url']
            if len(url) > 1000:
                url = url[:997] + "..."
            
            urls.append(url)
            titles.append(title)
            contents.append(content)
            content_types.append(block['content_type'])
            word_counts.append(block['word_count'])
            url_hashes.append(block['url_hash'])
            timestamps.append(block['timestamp'])
            embeddings.append(self.text_to_vector(content))
        
        # æ’å…¥æ•°æ®
        data = [urls, titles, contents, content_types, word_counts, url_hashes, timestamps, embeddings]
        
        try:
            insert_result = self.collection.insert(data)
            self.collection.flush()
            print(f"âœ… æˆåŠŸæ’å…¥ {insert_result.insert_count} ä¸ªå†…å®¹å—")
            return True
        except Exception as e:
            print(f"âŒ æ’å…¥æ•°æ®å¤±è´¥: {e}")
            return False
    
    def search_content(self, query: str, top_k: int = 5, content_type_filter: str = None) -> List[Dict]:
        """æœç´¢å†…å®¹"""
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: '{query}'")
        
        query_vector = self.text_to_vector(query)
        
        search_params = {
            "metric_type": "COSINE",
            "params": {"ef": 100}
        }
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        expr = None
        if content_type_filter:
            expr = f'content_type like "{content_type_filter}%"'
        
        try:
            results = self.collection.search(
                [query_vector],
                "embedding",
                search_params,
                limit=top_k,
                expr=expr,
                output_fields=["url", "title", "content", "content_type", "word_count"]
            )
            
            search_results = []
            for hits in results:
                for hit in hits:
                    result = {
                        "id": hit.id,
                        "score": float(hit.score),
                        "url": hit.entity.get("url"),
                        "title": hit.entity.get("title"),
                        "content": hit.entity.get("content"),
                        "content_type": hit.entity.get("content_type"),
                        "word_count": hit.entity.get("word_count")
                    }
                    search_results.append(result)
            
            return search_results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.collection:
            return {}
        
        try:
            stats = {
                "collection_name": self.collection.name,
                "total_blocks": self.collection.num_entities,
                "dimension": self.dimension,
                "has_semantic_model": HAS_SENTENCE_TRANSFORMERS
            }
            
            # è·å–å†…å®¹ç±»å‹åˆ†å¸ƒ
            content_types = self.collection.query(
                expr="id >= 0",
                output_fields=["content_type"],
                limit=1000
            )
            
            type_counts = {}
            for item in content_types:
                ct = item.get('content_type', 'unknown')
                type_counts[ct] = type_counts.get(ct, 0) + 1
            
            stats["content_type_distribution"] = type_counts
            return stats
            
        except Exception as e:
            print(f"âš ï¸  è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        if self.collection:
            self.collection.release()
        connections.disconnect("default")
        print("ğŸ”Œ å·²æ–­å¼€è¿æ¥")

def create_sample_html_files():
    """åˆ›å»ºç¤ºä¾‹ HTML æ–‡ä»¶ç”¨äºæµ‹è¯•"""
    html_files = {
        "tech_article.html": """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Python æœºå™¨å­¦ä¹ æŠ€æœ¯æŒ‡å—</title>
        </head>
        <body>
            <h1>Python æœºå™¨å­¦ä¹ å®Œæ•´æŒ‡å—</h1>
            <h2>ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ</h2>
            <p>æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œåšå‡ºå†³ç­–ã€‚é€šè¿‡ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œæœºå™¨å­¦ä¹ ç³»ç»Ÿå¯ä»¥ä»æ•°æ®ä¸­è¯†åˆ«æ¨¡å¼å¹¶åšå‡ºé¢„æµ‹ã€‚</p>
            
            <h2>Python åœ¨æœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŠ¿</h2>
            <ul>
                <li>ä¸°å¯Œçš„åº“ç”Ÿæ€ç³»ç»Ÿï¼šscikit-learnã€pandasã€numpy</li>
                <li>ç®€å•æ˜“å­¦çš„è¯­æ³•</li>
                <li>å¼ºå¤§çš„æ•°æ®å¤„ç†èƒ½åŠ›</li>
                <li>æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒ</li>
            </ul>
            
            <h3>å¸¸ç”¨æœºå™¨å­¦ä¹ åº“</h3>
            <table>
                <tr><th>åº“å</th><th>ç”¨é€”</th><th>ç‰¹ç‚¹</th></tr>
                <tr><td>scikit-learn</td><td>é€šç”¨æœºå™¨å­¦ä¹ </td><td>æ˜“ç”¨ã€æ–‡æ¡£å®Œå–„</td></tr>
                <tr><td>TensorFlow</td><td>æ·±åº¦å­¦ä¹ </td><td>è°·æ­Œå¼€å‘ã€åŠŸèƒ½å¼ºå¤§</td></tr>
                <tr><td>PyTorch</td><td>æ·±åº¦å­¦ä¹ </td><td>åŠ¨æ€å›¾ã€ç ”ç©¶å‹å¥½</td></tr>
            </table>
            
            <p>å¼€å§‹æœºå™¨å­¦ä¹ ä¹‹æ—…çš„ç¬¬ä¸€æ­¥æ˜¯ç†è§£æ•°æ®ã€‚æ•°æ®æ˜¯æœºå™¨å­¦ä¹ çš„ç‡ƒæ–™ï¼Œæ²¡æœ‰é«˜è´¨é‡çš„æ•°æ®ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ç®—æ³•ä¹Ÿæ— æ³•äº§ç”Ÿæœ‰æ„ä¹‰çš„ç»“æœã€‚æ•°æ®é¢„å¤„ç†åŒ…æ‹¬æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®è½¬æ¢ç­‰æ­¥éª¤ã€‚</p>
        </body>
        </html>
        """,
        
        "health_guide.html": """
        <!DOCTYPE html>
        <html>
        <head>
            <title>å¥åº·ç”Ÿæ´»æ–¹å¼æŒ‡å—</title>
        </head>
        <body>
            <h1>å¥åº·ç”Ÿæ´»æ–¹å¼å®Œå…¨æŒ‡å—</h1>
            
            <h2>å‡è¡¡é¥®é£Ÿçš„é‡è¦æ€§</h2>
            <p>å‡è¡¡é¥®é£Ÿæ˜¯å¥åº·ç”Ÿæ´»çš„åŸºçŸ³ã€‚å®ƒä¸ä»…ä¸ºæˆ‘ä»¬çš„èº«ä½“æä¾›å¿…è¦çš„è¥å…»ç´ ï¼Œè¿˜æœ‰åŠ©äºç»´æŒç†æƒ³çš„ä½“é‡å’Œé¢„é˜²æ…¢æ€§ç–¾ç—…ã€‚ä¸€ä¸ªå‡è¡¡çš„é¥®é£Ÿè®¡åˆ’åº”è¯¥åŒ…å«å„ç§é¢œè‰²çš„è”¬èœæ°´æœã€å…¨è°·ç‰©ã€ç˜¦è›‹ç™½è´¨å’Œå¥åº·è„‚è‚ªã€‚</p>
            
            <h3>æ¯æ—¥è¥å…»å»ºè®®</h3>
            <ul>
                <li>è”¬èœæ°´æœï¼šæ¯å¤©è‡³å°‘5ä»½ä¸åŒé¢œè‰²çš„è”¬æœ</li>
                <li>å…¨è°·ç‰©ï¼šé€‰æ‹©ç³™ç±³ã€å…¨éº¦é¢åŒ…ç­‰æœªç²¾åˆ¶è°·ç‰©</li>
                <li>è›‹ç™½è´¨ï¼šé±¼ç±»ã€è±†ç±»ã€åšæœå’Œé€‚é‡ç˜¦è‚‰</li>
                <li>å¥åº·è„‚è‚ªï¼šæ©„æ¦„æ²¹ã€é³„æ¢¨ã€åšæœä¸­çš„ä¸é¥±å’Œè„‚è‚ª</li>
                <li>æ°´åˆ†ï¼šæ¯å¤©è‡³å°‘8æ¯æ°´</li>
            </ul>
            
            <h2>è§„å¾‹è¿åŠ¨çš„ç›Šå¤„</h2>
            <p>è¿åŠ¨ä¸ä»…èƒ½å¸®åŠ©æˆ‘ä»¬ä¿æŒå¥åº·çš„ä½“é‡ï¼Œè¿˜èƒ½å¢å¼ºå¿ƒè¡€ç®¡åŠŸèƒ½ã€æé«˜å…ç–«åŠ›ã€æ”¹å–„å¿ƒç†å¥åº·ã€‚ä¸–ç•Œå«ç”Ÿç»„ç»‡å»ºè®®æˆå¹´äººæ¯å‘¨è‡³å°‘è¿›è¡Œ150åˆ†é’Ÿçš„ä¸­ç­‰å¼ºåº¦æœ‰æ°§è¿åŠ¨ã€‚</p>
            
            <h3>æ¨èçš„è¿åŠ¨ç±»å‹</h3>
            <ol>
                <li>æœ‰æ°§è¿åŠ¨ï¼šè·‘æ­¥ã€æ¸¸æ³³ã€éª‘è‡ªè¡Œè½¦</li>
                <li>åŠ›é‡è®­ç»ƒï¼šä¸¾é‡ã€ä¿¯å§æ’‘ã€æ·±è¹²</li>
                <li>æŸ”éŸ§æ€§è®­ç»ƒï¼šç‘œä¼½ã€å¤ªæã€æ‹‰ä¼¸</li>
                <li>å¹³è¡¡è®­ç»ƒï¼šå•è„šç«™ç«‹ã€å¹³è¡¡æ¿ç»ƒä¹ </li>
            </ol>
        </body>
        </html>
        """,
        
        "cooking_tips.html": """
        <!DOCTYPE html>
        <html>
        <head>
            <title>çƒ¹é¥ªæŠ€å·§å¤§å…¨</title>
        </head>
        <body>
            <h1>çƒ¹é¥ªæŠ€å·§ä¸ç¾é£Ÿåˆ¶ä½œ</h1>
            
            <h2>åŸºæœ¬çƒ¹é¥ªæŠ€æ³•</h2>
            <p>æŒæ¡åŸºæœ¬çš„çƒ¹é¥ªæŠ€æ³•æ˜¯æˆä¸ºä¼˜ç§€å¨å¸ˆçš„ç¬¬ä¸€æ­¥ã€‚ä¸åŒçš„çƒ¹é¥ªæ–¹æ³•ä¼šäº§ç”Ÿä¸åŒçš„å£æ„Ÿå’Œé£å‘³ï¼Œäº†è§£æ¯ç§æŠ€æ³•çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯éå¸¸é‡è¦ã€‚</p>
            
            <h3>ä¸­å¼çƒ¹é¥ªæŠ€æ³•</h3>
            <ul>
                <li>ç‚’ï¼šé«˜æ¸©å¿«é€Ÿçƒ¹é¥ªï¼Œä¿æŒé£Ÿæè„†å«©</li>
                <li>ç…®ï¼šç”¨æ°´æˆ–æ±¤æ±åŠ çƒ­ï¼Œé€‚åˆåˆ¶ä½œæ±¤å“</li>
                <li>è’¸ï¼šåˆ©ç”¨è’¸æ±½åŠ çƒ­ï¼Œä¿æŒé£ŸæåŸå‘³</li>
                <li>ç‚–ï¼šé•¿æ—¶é—´å°ç«æ…¢ç…®ï¼Œä½¿é£Ÿæè½¯çƒ‚å…¥å‘³</li>
                <li>çƒ§ï¼šè°ƒå‘³åç„–ç…®ï¼Œæ±æµ“å‘³åš</li>
            </ul>
            
            <h2>é£Ÿæé€‰æ‹©ä¸æ­é…</h2>
            <p>æ–°é²œçš„é£Ÿææ˜¯ç¾å‘³ä½³è‚´çš„åŸºç¡€ã€‚é€‰æ‹©é£Ÿææ—¶è¦æ³¨æ„è§‚å¯Ÿå…¶å¤–è§‚ã€æ°”å‘³å’Œè§¦æ„Ÿã€‚ä¸åŒçš„é£Ÿææœ‰ä¸åŒçš„æœ€ä½³æ­é…ï¼Œåˆç†çš„æ­é…ä¸ä»…èƒ½æå‡å£å‘³ï¼Œè¿˜èƒ½å¢åŠ è¥å…»ä»·å€¼ã€‚</p>
            
            <table>
                <tr><th>é£Ÿæç±»å‹</th><th>é€‰æ‹©è¦ç‚¹</th><th>æœ€ä½³æ­é…</th></tr>
                <tr><td>è”¬èœ</td><td>è‰²æ³½é²œè‰³ã€è´¨åœ°è„†å«©</td><td>è¤ç´ æ­é…ã€é¢œè‰²æ­é…</td></tr>
                <tr><td>è‚‰ç±»</td><td>æ— å¼‚å‘³ã€å¼¹æ€§å¥½</td><td>é¦™æ–™è°ƒå‘³ã€è”¬èœå¹³è¡¡</td></tr>
                <tr><td>æµ·é²œ</td><td>çœ¼ç›æ˜äº®ã€é³ƒè‰²é²œçº¢</td><td>æŸ æª¬å»è…¥ã€æ¸…æ·¡è°ƒå‘³</td></tr>
            </table>
            
            <h3>è°ƒå‘³çš„è‰ºæœ¯</h3>
            <p>è°ƒå‘³æ˜¯çƒ¹é¥ªçš„çµé­‚ã€‚åˆé€‚çš„è°ƒå‘³èƒ½å¤Ÿçªå‡ºé£Ÿæçš„å¤©ç„¶å‘³é“ï¼Œè€Œä¸æ˜¯æ©ç›–å®ƒã€‚è°ƒå‘³è¦éµå¾ªå¾ªåºæ¸è¿›çš„åŸåˆ™ï¼Œå…ˆå°‘é‡æ·»åŠ ï¼Œå“å°åå†è°ƒæ•´ã€‚è®°ä½ï¼Œç›è¦åæ”¾ï¼Œä»¥å…å½±å“å…¶ä»–è°ƒæ–™çš„å¸æ”¶ã€‚</p>
        </body>
        </html>
        """
    }
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶ç›®å½•
    os.makedirs("sample_html", exist_ok=True)
    
    for filename, content in html_files.items():
        filepath = os.path.join("sample_html", filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"ğŸ“„ åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {filepath}")
    
    return list(html_files.keys())

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤º HTML å†…å®¹å‘é‡åŒ–å­˜å‚¨"""
    print("ğŸŒ HTML å†…å®¹å‘é‡åŒ–å­˜å‚¨ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = HTMLToMilvusProcessor()
    
    try:
        # 1. è¿æ¥åˆ° Milvus
        if not processor.connect_to_milvus():
            return
        
        # 2. åˆ›å»ºé›†åˆ
        processor.create_collection()
        processor.create_index()
        processor.load_collection()
        
        # 3. åˆ›å»ºç¤ºä¾‹ HTML æ–‡ä»¶
        print("\nğŸ“„ åˆ›å»ºç¤ºä¾‹ HTML æ–‡ä»¶...")
        sample_files = create_sample_html_files()
        
        # 4. å¤„ç†æœ¬åœ° HTML æ–‡ä»¶
        all_content_blocks = []
        for filename in sample_files:
            filepath = os.path.join("sample_html", filename)
            print(f"\nğŸ”„ å¤„ç†æ–‡ä»¶: {filepath}")
            
            # è¯»å– HTML æ–‡ä»¶
            with open(filepath, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # è§£æ HTML
            content_blocks = processor.parse_html(html_content, base_url=filepath)
            all_content_blocks.extend(content_blocks)
        
        # 5. æ’å…¥åˆ° Milvus
        processor.insert_html_content(all_content_blocks)
        
        # 6. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯:")
        stats = processor.get_statistics()
        print(f"   é›†åˆåç§°: {stats.get('collection_name', 'N/A')}")
        print(f"   å†…å®¹å—æ€»æ•°: {stats.get('total_blocks', 'N/A')}")
        print(f"   å‘é‡ç»´åº¦: {stats.get('dimension', 'N/A')}")
        print(f"   è¯­ä¹‰æ¨¡å‹: {'âœ… å·²å¯ç”¨' if stats.get('has_semantic_model') else 'âŒ ç®€åŒ–æ¨¡å¼'}")
        
        if 'content_type_distribution' in stats:
            print("   å†…å®¹ç±»å‹åˆ†å¸ƒ:")
            for content_type, count in stats['content_type_distribution'].items():
                print(f"     - {content_type}: {count}")
        
        # 7. æ¼”ç¤ºæœç´¢åŠŸèƒ½
        print(f"\nğŸ” æœç´¢æ¼”ç¤º:")
        print("-" * 40)
        
        test_queries = [
            "æœºå™¨å­¦ä¹ ç®—æ³•",
            "å¥åº·é¥®é£Ÿå»ºè®®", 
            "çƒ¹é¥ªæŠ€å·§",
            "Python ç¼–ç¨‹",
            "è¿åŠ¨é”»ç‚¼æ–¹æ³•"
        ]
        
        for query in test_queries:
            print(f"\nğŸ¯ æŸ¥è¯¢: '{query}'")
            results = processor.search_content(query, top_k=3)
            
            if results:
                for i, result in enumerate(results, 1):
                    print(f"  {i}. ã€{result['content_type']}ã€‘ç›¸ä¼¼åº¦: {result['score']:.4f}")
                    print(f"      æ¥æº: {result['url']}")
                    print(f"      å†…å®¹: {result['content'][:80]}...")
            else:
                print("  æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        
        # 8. æŒ‰å†…å®¹ç±»å‹æœç´¢
        print(f"\nğŸ·ï¸  æŒ‰å†…å®¹ç±»å‹æœç´¢æ¼”ç¤º:")
        print("-" * 40)
        
        query = "è¥å…»å¥åº·"
        content_type = "paragraph"
        print(f"åœ¨å†…å®¹ç±»å‹ '{content_type}' ä¸­æœç´¢: '{query}'")
        
        filtered_results = processor.search_content(query, top_k=3, content_type_filter=content_type)
        for i, result in enumerate(filtered_results, 1):
            print(f"  {i}. {result['content'][:100]}... (åˆ†æ•°: {result['score']:.4f})")
        
        print(f"\nâœ… HTML å†…å®¹å‘é‡åŒ–å­˜å‚¨æ¼”ç¤ºå®Œæˆ!")
        
        # 9. æä¾›è¿›ä¸€æ­¥çš„ä½¿ç”¨å»ºè®®
        print(f"\nğŸ’¡ è¿›ä¸€æ­¥ä½¿ç”¨å»ºè®®:")
        print("   1. ä¿®æ”¹ sample_html/ ç›®å½•ä¸‹çš„ HTML æ–‡ä»¶æ¥æµ‹è¯•ä¸åŒå†…å®¹")
        print("   2. åœ¨ main() å‡½æ•°ä¸­æ·»åŠ ç½‘ç»œçˆ¬å–åŠŸèƒ½ (ä½¿ç”¨ processor.fetch_html())")
        print("   3. è°ƒæ•´å‘é‡åŒ–æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„è¯­ä¹‰ç†è§£")
        print("   4. æ·»åŠ æ›´å¤šçš„å†…å®¹ç±»å‹å’Œè¿‡æ»¤æ¡ä»¶")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        processor.disconnect()

def demo_web_crawling():
    """æ¼”ç¤ºä»ç½‘ç»œçˆ¬å– HTML å†…å®¹ (å¯é€‰åŠŸèƒ½)"""
    print("ğŸŒ ç½‘ç»œçˆ¬å–æ¼”ç¤º (è¯·ç¡®ä¿ç›®æ ‡ç½‘ç«™å…è®¸çˆ¬å–)")
    
    # ç¤ºä¾‹ç½‘ç«™åˆ—è¡¨ (è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)
    urls = [
        "https://disclosure2.edinet-fsa.go.jp/WZEK0040.aspx?S100FWD6,,",  # è¯·æ›¿æ¢ä¸ºå®é™…çš„ç½‘ç«™URL
        # "https://your-blog.com/article1",
        # "https://docs.python.org/3/tutorial/",
    ]
    
    processor = HTMLToMilvusProcessor(collection_name="web_content")
    
    try:
        processor.connect_to_milvus()
        processor.create_collection()
        processor.create_index()
        processor.load_collection()
        
        all_content_blocks = []
        
        for url in urls:
            html_content = processor.fetch_html(url)
            if html_content:
                content_blocks = processor.parse_html(html_content, base_url=url)
                all_content_blocks.extend(content_blocks)
        
        if all_content_blocks:
            processor.insert_html_content(all_content_blocks)
            print("âœ… ç½‘ç»œå†…å®¹çˆ¬å–å’Œå­˜å‚¨å®Œæˆ")
        else:
            print("âš ï¸  æ²¡æœ‰æˆåŠŸè·å–ä»»ä½•ç½‘ç»œå†…å®¹")
            
    except Exception as e:
        print(f"âŒ ç½‘ç»œçˆ¬å–å¤±è´¥: {e}")
    finally:
        processor.disconnect()

if __name__ == "__main__":
    # main()
    
    # å¦‚æœéœ€è¦æµ‹è¯•ç½‘ç»œçˆ¬å–åŠŸèƒ½ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # print("\n" + "="*60)
    demo_web_crawling() 