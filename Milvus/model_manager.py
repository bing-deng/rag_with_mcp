#!/usr/bin/env python3
"""
æ¨¡å‹ç®¡ç†å™¨ - å•ä¾‹æ¨¡å¼
é¿å…é‡å¤åŠ è½½Sentence-Transformerså’ŒOllamaè¿æ¥
"""

import threading
import requests
from typing import Optional, List
import os

# å°è¯•å¯¼å…¥Sentence-Transformers
try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("âš ï¸  sentence-transformers æœªå®‰è£…")

class ModelManager:
    """å•ä¾‹æ¨¡å¼çš„æ¨¡å‹ç®¡ç†å™¨"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self._sentence_model = None
            self._ollama_base_url = "http://localhost:11434"
            self._ollama_available = False
            # å‘é‡ç¼“å­˜
            self._vector_cache = {}
            self._cache_size_limit = 1000  # ç¼“å­˜1000ä¸ªå‘é‡
            self._initialized = True
            print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨ (å¸¦ç¼“å­˜)...")
    
    def get_sentence_model(self) -> Optional[SentenceTransformer]:
        """è·å–Sentence-Transformersæ¨¡å‹ - è¶…çº§ä¼˜åŒ–ç‰ˆ"""
        if self._sentence_model is None and HAS_SENTENCE_TRANSFORMERS:
            try:
                print("ğŸ”§ åŠ è½½è¯­ä¹‰å‘é‡åŒ–æ¨¡å‹ (ä»…æ­¤ä¸€æ¬¡)...")
                # ä½¿ç”¨æ›´å°æ›´å¿«çš„æ¨¡å‹ï¼Œæ˜ç¡®ç¦ç”¨GPUæ£€æµ‹
                import os
                os.environ['CUDA_VISIBLE_DEVICES'] = ''  # å¼ºåˆ¶CPU
                
                self._sentence_model = SentenceTransformer(
                    'paraphrase-multilingual-MiniLM-L12-v2',
                    device='cpu'
                )
                # è¿›ä¸€æ­¥ä¼˜åŒ–å‚æ•°
                self._sentence_model.max_seq_length = 128  # å¤§å¹…å‡å°‘åºåˆ—é•¿åº¦
                
                # ç¦ç”¨ä¸€äº›å¯èƒ½çš„å¼€é”€
                if hasattr(self._sentence_model, '_modules'):
                    for module in self._sentence_model._modules.values():
                        if hasattr(module, 'gradient_checkpointing'):
                            module.gradient_checkpointing = False
                
                # é¢„çƒ­æ¨¡å‹
                print("ğŸ”¥ é¢„çƒ­å‘é‡æ¨¡å‹...")
                _ = self._sentence_model.encode("é¢„çƒ­", normalize_embeddings=True)
                print("âœ… è¯­ä¹‰æ¨¡å‹åŠ è½½å®Œæˆå¹¶ç¼“å­˜")
            except Exception as e:
                print(f"âŒ è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return None
        return self._sentence_model
    
    def check_ollama_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(f"{self._ollama_base_url}/api/tags", timeout=3)
            if response.status_code == 200:
                models = response.json().get('models', [])
                available_models = [model['name'] for model in models]
                if available_models:
                    self._ollama_available = True
                    print(f"âœ… Ollama æœåŠ¡å¯ç”¨ï¼Œæ¨¡å‹: {available_models}")
                    return True
            return False
        except Exception as e:
            self._ollama_available = False
            print(f"âŒ Ollama æœåŠ¡ä¸å¯ç”¨: {e}")
            return False
    
    def is_ollama_available(self) -> bool:
        """Ollamaæ˜¯å¦å¯ç”¨"""
        return self._ollama_available
    
    def generate_with_ollama(self, prompt: str, model_name: str = 'llama3.2:3b', max_tokens: int = 500) -> str:
        """ä½¿ç”¨Ollamaç”Ÿæˆå›ç­” - è¶…çº§ä¼˜åŒ–ç‰ˆ"""
        # åªåœ¨ç¬¬ä¸€æ¬¡æˆ–å¤±è´¥æ—¶æ£€æŸ¥æœåŠ¡
        if not self._ollama_available:
            if not self.check_ollama_service():
                return "âŒ OllamaæœåŠ¡ä¸å¯ç”¨"
        
        try:
            data = {
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.05,    # è¿›ä¸€æ­¥é™ä½éšæœºæ€§
                    "top_k": 5,            # æ›´ä¸¥æ ¼é™åˆ¶å€™é€‰è¯
                    "top_p": 0.8,          # æ›´ä¸¥æ ¼çš„æ ¸é‡‡æ ·
                    "repeat_penalty": 1.05, # å‡å°‘é‡å¤æ£€æŸ¥å¼€é”€
                    "num_thread": 4,       # é™åˆ¶çº¿ç¨‹æ•°
                    "num_ctx": 1024,       # å‡å°‘ä¸Šä¸‹æ–‡çª—å£
                    "batch_size": 1,       # å•æ‰¹å¤„ç†
                    "seed": 42             # å›ºå®šç§å­ï¼Œå‡å°‘éšæœºæ€§å¼€é”€
                }
            }
            
            response = requests.post(
                f"{self._ollama_base_url}/api/generate",
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                return f"âŒ Ollama è¯·æ±‚å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            return f"âŒ Ollama ç”Ÿæˆå¤±è´¥: {e}"
    
    def text_to_vector(self, text: str) -> Optional[List[float]]:
        """æ–‡æœ¬è½¬å‘é‡ - å¸¦ç¼“å­˜"""
        # æ£€æŸ¥ç¼“å­˜
        if text in self._vector_cache:
            return self._vector_cache[text]
        
        model = self.get_sentence_model()
        if model:
            try:
                embedding = model.encode(text, normalize_embeddings=True)
                vector = embedding.tolist()
                
                # ç¼“å­˜å‘é‡ï¼Œä½†é™åˆ¶ç¼“å­˜å¤§å°
                if len(self._vector_cache) >= self._cache_size_limit:
                    # ç®€å•çš„FIFOç¼“å­˜æ¸…ç†
                    oldest_key = next(iter(self._vector_cache))
                    del self._vector_cache[oldest_key]
                
                self._vector_cache[text] = vector
                return vector
            except Exception as e:
                print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
                return None
        return None
    
    def warm_up(self):
        """é¢„çƒ­æ¨¡å‹"""
        print("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        # é¢„çƒ­Sentence-Transformers
        if self.get_sentence_model():
            self.text_to_vector("test")
            print("âœ… å‘é‡æ¨¡å‹é¢„çƒ­å®Œæˆ")
        
        # æ£€æŸ¥Ollama
        if self.check_ollama_service():
            print("âœ… OllamaæœåŠ¡æ£€æŸ¥å®Œæˆ")

# å…¨å±€å•ä¾‹å®ä¾‹
_model_manager = ModelManager()

def get_model_manager() -> ModelManager:
    """è·å–æ¨¡å‹ç®¡ç†å™¨å•ä¾‹"""
    return _model_manager