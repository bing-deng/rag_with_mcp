#!/usr/bin/env python3
"""
å®Œæ•´ç½‘ç«™çˆ¬è™«ç³»ç»Ÿ
çˆ¬å–æ•´ä¸ªç½‘ç«™çš„æ‰€æœ‰é¡µé¢ï¼Œè§£æå†…å®¹å¹¶å­˜å‚¨åˆ° Milvus å‘é‡æ•°æ®åº“ä¸­
æ”¯æŒå¤šçº¿ç¨‹ã€å»é‡ã€é”™è¯¯å¤„ç†ç­‰é«˜çº§åŠŸèƒ½
"""

import os
import re
import time
import hashlib
import requests
import threading
from queue import Queue, Empty
from urllib.parse import urljoin, urlparse, urlencode
from urllib.robotparser import RobotFileParser
from typing import List, Dict, Set, Optional
from dataclasses import dataclass, asdict
import json
from datetime import datetime

# HTML è§£æ
from bs4 import BeautifulSoup

# å¹¶å‘å¤„ç†
from concurrent.futures import ThreadPoolExecutor, as_completed

# Milvus å’Œå‘é‡åŒ–
from html_to_milvus import HTMLToMilvusProcessor

@dataclass
class CrawlConfig:
    """çˆ¬è™«é…ç½®"""
    base_url: str
    max_pages: int = 1000
    max_depth: int = 3
    concurrent_workers: int = 5
    delay_between_requests: float = 1.0
    timeout: int = 30
    respect_robots_txt: bool = True
    allowed_domains: List[str] = None
    exclude_patterns: List[str] = None
    include_patterns: List[str] = None
    collection_name: str = "website_content"

class WebsiteCrawler:
    """å®Œæ•´ç½‘ç«™çˆ¬è™«"""
    
    def __init__(self, config: CrawlConfig):
        self.config = config
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()
        self.url_queue = Queue()
        self.results = []
        self.lock = threading.Lock()
        
        # è§£æåŸºç¡€åŸŸå
        parsed = urlparse(config.base_url)
        self.base_domain = parsed.netloc
        self.base_scheme = parsed.scheme
        
        # å…è®¸çš„åŸŸå
        if config.allowed_domains is None:
            self.allowed_domains = {self.base_domain}
        else:
            self.allowed_domains = set(config.allowed_domains)
            self.allowed_domains.add(self.base_domain)
        
        # åˆå§‹åŒ– robots.txt æ£€æŸ¥
        self.rp = None
        if config.respect_robots_txt:
            self._load_robots_txt()
        
        # åˆå§‹åŒ– Milvus å¤„ç†å™¨
        self.milvus_processor = HTMLToMilvusProcessor(
            collection_name=config.collection_name
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'pages_crawled': 0,
            'pages_failed': 0,
            'content_blocks_extracted': 0,
            'start_time': None,
            'end_time': None
        }
    
    def _load_robots_txt(self):
        """åŠ è½½å¹¶è§£æ robots.txt"""
        try:
            robots_url = urljoin(self.config.base_url, '/robots.txt')
            self.rp = RobotFileParser()
            self.rp.set_url(robots_url)
            self.rp.read()
            print(f"âœ… å·²åŠ è½½ robots.txt: {robots_url}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½ robots.txt: {e}")
            self.rp = None
    
    def _can_fetch(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥çˆ¬å–è¯¥URL"""
        if self.rp is None:
            return True
        return self.rp.can_fetch('*', url)
    
    def _is_valid_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or url in self.visited_urls:
            return False
        
        parsed = urlparse(url)
        
        # æ£€æŸ¥åŸŸå
        if parsed.netloc not in self.allowed_domains:
            return False
        
        # æ£€æŸ¥åè®®
        if parsed.scheme not in ['http', 'https']:
            return False
        
        # æ£€æŸ¥æ’é™¤æ¨¡å¼
        if self.config.exclude_patterns:
            for pattern in self.config.exclude_patterns:
                if re.search(pattern, url):
                    return False
        
        # æ£€æŸ¥åŒ…å«æ¨¡å¼
        if self.config.include_patterns:
            for pattern in self.config.include_patterns:
                if re.search(pattern, url):
                    return True
            return False
        
        # æ’é™¤å¸¸è§çš„éå†…å®¹URL
        exclude_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.rar', '.exe']
        if any(url.lower().endswith(ext) for ext in exclude_extensions):
            return False
        
        return True
    
    def _extract_links(self, html_content: str, base_url: str) -> List[str]:
        """ä»HTMLä¸­æå–é“¾æ¥"""
        links = []
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ‰€æœ‰é“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link['href'].strip()
                if not href:
                    continue
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                absolute_url = urljoin(base_url, href)
                
                # ç§»é™¤é”šç‚¹å’ŒæŸ¥è¯¢å‚æ•°ï¼ˆå¯é€‰ï¼‰
                parsed = urlparse(absolute_url)
                clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                
                if self._is_valid_url(clean_url):
                    links.append(clean_url)
        
        except Exception as e:
            print(f"âš ï¸  æå–é“¾æ¥å¤±è´¥: {e}")
        
        return list(set(links))  # å»é‡
    
    def _fetch_page(self, url: str) -> Optional[Dict]:
        """è·å–å•ä¸ªé¡µé¢"""
        if not self._can_fetch(url):
            print(f"ğŸš« Robots.txt ç¦æ­¢è®¿é—®: {url}")
            return None
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ja,en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            response = requests.get(
                url, 
                headers=headers, 
                timeout=self.config.timeout,
                allow_redirects=True
            )
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' not in content_type:
                print(f"âš ï¸  è·³è¿‡éHTMLå†…å®¹: {url} ({content_type})")
                return None
            
            # ç¡®ä¿æ­£ç¡®çš„ç¼–ç å¤„ç†ï¼Œç‰¹åˆ«æ˜¯æ—¥æ–‡å†…å®¹
            if response.encoding is None or response.encoding.lower() in ['iso-8859-1', 'ascii']:
                response.encoding = response.apparent_encoding or 'utf-8'
            
            return {
                'url': url,
                'content': response.text,
                'status_code': response.status_code,
                'content_length': len(response.text),
                'timestamp': time.time()
            }
        
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥: {url} - {e}")
            with self.lock:
                self.failed_urls.add(url)
                self.stats['pages_failed'] += 1
            return None
    
    def _process_page(self, page_data: Dict, depth: int) -> List[str]:
        """å¤„ç†å•ä¸ªé¡µé¢ï¼Œè¿”å›å‘ç°çš„æ–°é“¾æ¥"""
        url = page_data['url']
        html_content = page_data['content']
        
        print(f"ğŸ“„ å¤„ç†é¡µé¢ (æ·±åº¦ {depth}): {url}")
        
        try:
            # è§£æHTMLå†…å®¹
            content_blocks = self.milvus_processor.parse_html(html_content, base_url=url)
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            with self.lock:
                self.stats['pages_crawled'] += 1
                self.stats['content_blocks_extracted'] += len(content_blocks)
                self.visited_urls.add(url)
            
            # æå–é“¾æ¥
            new_links = []
            if depth < self.config.max_depth:
                links = self._extract_links(html_content, url)
                new_links = [link for link in links if link not in self.visited_urls]
            
            # å­˜å‚¨é¡µé¢ç»“æœ
            self.results.append({
                'url': url,
                'content_blocks': content_blocks,
                'links_found': len(new_links),
                'depth': depth,
                'timestamp': page_data['timestamp'],
                'content_length': page_data['content_length']
            })
            
            return new_links
        
        except Exception as e:
            print(f"âŒ å¤„ç†é¡µé¢å¤±è´¥: {url} - {e}")
            return []
    
    def _worker(self, depth: int):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        while True:
            try:
                url = self.url_queue.get(timeout=10)
                
                # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
                with self.lock:
                    if url in self.visited_urls:
                        self.url_queue.task_done()
                        continue
                
                # å»¶è¿Ÿè¯·æ±‚
                time.sleep(self.config.delay_between_requests)
                
                # è·å–é¡µé¢
                page_data = self._fetch_page(url)
                if page_data is None:
                    self.url_queue.task_done()
                    continue
                
                # å¤„ç†é¡µé¢
                new_links = self._process_page(page_data, depth)
                
                # æ·»åŠ æ–°é“¾æ¥åˆ°é˜Ÿåˆ—
                if depth < self.config.max_depth:
                    for link in new_links:
                        with self.lock:
                            if (link not in self.visited_urls and 
                                len(self.visited_urls) < self.config.max_pages):
                                self.url_queue.put(link)
                
                self.url_queue.task_done()
                
            except Empty:
                break
            except Exception as e:
                print(f"âŒ å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
                self.url_queue.task_done()
    
    def crawl(self) -> Dict:
        """å¼€å§‹çˆ¬å–ç½‘ç«™"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–ç½‘ç«™: {self.config.base_url}")
        print(f"ğŸ“Š é…ç½®: æœ€å¤§é¡µé¢æ•°={self.config.max_pages}, æœ€å¤§æ·±åº¦={self.config.max_depth}, å¹¶å‘æ•°={self.config.concurrent_workers}")
        
        self.stats['start_time'] = time.time()
        
        # æ·»åŠ èµ·å§‹URL
        self.url_queue.put(self.config.base_url)
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        threads = []
        for i in range(self.config.concurrent_workers):
            t = threading.Thread(target=self._worker, args=(0,))
            t.daemon = True
            t.start()
            threads.append(t)
        
        try:
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            self.url_queue.join()
            
            # å¤„ç†æ›´æ·±å±‚æ¬¡çš„é“¾æ¥
            for depth in range(1, self.config.max_depth + 1):
                if len(self.visited_urls) >= self.config.max_pages:
                    break
                
                print(f"\nğŸ” å¼€å§‹å¤„ç†æ·±åº¦ {depth} çš„é¡µé¢...")
                
                # å¯åŠ¨æ–°çš„å·¥ä½œçº¿ç¨‹å¤„ç†è¿™ä¸ªæ·±åº¦
                depth_threads = []
                for i in range(self.config.concurrent_workers):
                    t = threading.Thread(target=self._worker, args=(depth,))
                    t.daemon = True
                    t.start()
                    depth_threads.append(t)
                
                # ç­‰å¾…è¿™ä¸ªæ·±åº¦çš„ä»»åŠ¡å®Œæˆ
                self.url_queue.join()
                
                # ç­‰å¾…çº¿ç¨‹ç»“æŸ
                for t in depth_threads:
                    t.join(timeout=1)
        
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–è¿‡ç¨‹")
        
        self.stats['end_time'] = time.time()
        
        return self._generate_report()
    
    def _generate_report(self) -> Dict:
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        report = {
            'config': asdict(self.config),
            'stats': {
                **self.stats,
                'duration_seconds': duration,
                'pages_per_second': self.stats['pages_crawled'] / duration if duration > 0 else 0,
                'total_urls_discovered': len(self.visited_urls) + len(self.failed_urls),
                'success_rate': self.stats['pages_crawled'] / (self.stats['pages_crawled'] + self.stats['pages_failed']) if (self.stats['pages_crawled'] + self.stats['pages_failed']) > 0 else 0
            },
            'visited_urls': list(self.visited_urls),
            'failed_urls': list(self.failed_urls),
            'sample_results': self.results[:5]  # åªä¿å­˜å‰5ä¸ªç»“æœä½œä¸ºç¤ºä¾‹
        }
        
        return report
    
    def save_to_milvus(self) -> bool:
        """å°†çˆ¬å–çš„å†…å®¹ä¿å­˜åˆ° Milvus"""
        if not self.results:
            print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return False
        
        print(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®åˆ° Milvus...")
        
        try:
            # è¿æ¥åˆ° Milvus
            if not self.milvus_processor.connect_to_milvus():
                return False
            
            # åˆ›å»ºé›†åˆ
            self.milvus_processor.create_collection()
            self.milvus_processor.create_index()
            self.milvus_processor.load_collection()
            
            # æ”¶é›†æ‰€æœ‰å†…å®¹å—
            all_content_blocks = []
            for result in self.results:
                all_content_blocks.extend(result['content_blocks'])
            
            # æ‰¹é‡æ’å…¥åˆ° Milvus
            success = self.milvus_processor.insert_html_content(all_content_blocks)
            
            if success:
                print(f"âœ… æˆåŠŸä¿å­˜ {len(all_content_blocks)} ä¸ªå†…å®¹å—åˆ° Milvus")
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                stats = self.milvus_processor.get_statistics()
                print(f"ğŸ“Š Milvus ç»Ÿè®¡ä¿¡æ¯:")
                print(f"   é›†åˆåç§°: {stats.get('collection_name')}")
                print(f"   æ€»å†…å®¹å—: {stats.get('total_blocks')}")
                
                if 'content_type_distribution' in stats:
                    print("   å†…å®¹ç±»å‹åˆ†å¸ƒ:")
                    for content_type, count in stats['content_type_distribution'].items():
                        print(f"     {content_type}: {count}")
            
            return success
            
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ° Milvus å¤±è´¥: {e}")
            return False
        
        finally:
            self.milvus_processor.disconnect()
    
    def save_report(self, filename: str = None) -> str:
        """ä¿å­˜çˆ¬å–æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            domain = urlparse(self.config.base_url).netloc.replace('.', '_')
            filename = f"crawl_report_{domain}_{timestamp}.json"
        
        report = self._generate_report()
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ğŸ“‹ çˆ¬å–æŠ¥å‘Šå·²ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            return ""

def create_kandenko_config() -> CrawlConfig:
    """ä¸º Kandenko ç½‘ç«™åˆ›å»ºä¸“ç”¨é…ç½®"""
    config = CrawlConfig(
        base_url="https://www.kandenko.co.jp/",
        max_pages=200,  # é™åˆ¶é¡µé¢æ•°é‡ï¼Œé¿å…è¿‡åº¦çˆ¬å–
        max_depth=3,    # çˆ¬å–æ·±åº¦
        concurrent_workers=3,  # å¹¶å‘æ•°ï¼Œä¸è¦å¤ªé«˜ä»¥å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
        delay_between_requests=2.0,  # è¯·æ±‚é—´éš”2ç§’ï¼Œæ›´ç¤¼è²Œ
        timeout=30,
        respect_robots_txt=True,
        collection_name="kandenko_website",
        # åªçˆ¬å–åŒåŸŸåä¸‹çš„é¡µé¢
        allowed_domains=["www.kandenko.co.jp"],
        # æ’é™¤ä¸€äº›ä¸éœ€è¦çš„é¡µé¢ç±»å‹
        exclude_patterns=[
            r'/print/',  # æ‰“å°é¡µé¢
            r'/search\?',  # æœç´¢ç»“æœé¡µ
            r'\.pdf$',   # PDFæ–‡ä»¶
            r'/contact/.*send',  # è¡¨å•æäº¤é¡µé¢
            r'/admin/',  # ç®¡ç†é¡µé¢
        ]
    )
    return config

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ•·ï¸  å®Œæ•´ç½‘ç«™çˆ¬è™«ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»º Kandenko ä¸“ç”¨é…ç½®
    config = create_kandenko_config()
    
    print(f"ğŸ¯ ç›®æ ‡ç½‘ç«™: {config.base_url}")
    print(f"ğŸ“Š çˆ¬å–é…ç½®:")
    print(f"   æœ€å¤§é¡µé¢æ•°: {config.max_pages}")
    print(f"   æœ€å¤§æ·±åº¦: {config.max_depth}")
    print(f"   å¹¶å‘å·¥ä½œå™¨: {config.concurrent_workers}")
    print(f"   è¯·æ±‚é—´éš”: {config.delay_between_requests}ç§’")
    
    # ç¡®è®¤å¼€å§‹
    confirm = input(f"\næ˜¯å¦å¼€å§‹çˆ¬å– {config.base_url}ï¼Ÿ(y/N): ").strip().lower()
    if confirm != 'y':
        print("âŒ ç”¨æˆ·å–æ¶ˆ")
        return
    
    # åˆ›å»ºçˆ¬è™«
    crawler = WebsiteCrawler(config)
    
    try:
        # å¼€å§‹çˆ¬å–
        report = crawler.crawl()
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“‹ çˆ¬å–å®ŒæˆæŠ¥å‘Š:")
        print(f"   çˆ¬å–è€—æ—¶: {report['stats']['duration_seconds']:.2f} ç§’")
        print(f"   æˆåŠŸé¡µé¢: {report['stats']['pages_crawled']}")
        print(f"   å¤±è´¥é¡µé¢: {report['stats']['pages_failed']}")
        print(f"   å†…å®¹å—æ•°: {report['stats']['content_blocks_extracted']}")
        print(f"   æˆåŠŸç‡: {report['stats']['success_rate']:.2%}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = crawler.save_report()
        
        # ä¿å­˜åˆ° Milvus
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ° Milvus...")
        if crawler.save_to_milvus():
            print(f"âœ… æ•°æ®å·²æˆåŠŸä¿å­˜åˆ° Milvus é›†åˆ: {config.collection_name}")
            print(f"\nğŸ‰ ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œ AI æŸ¥è¯¢:")
            print(f"   python query_milvus.py  # ä½¿ç”¨é›†åˆåç§°: {config.collection_name}")
            print(f"   python llama_query.py   # LLaMA æ™ºèƒ½é—®ç­”")
        else:
            print(f"âŒ ä¿å­˜åˆ° Milvus å¤±è´¥")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 