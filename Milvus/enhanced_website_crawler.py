#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆç½‘ç«™çˆ¬è™«
é›†æˆæ™ºèƒ½åˆ†å—å¤„ç†ï¼Œä¸“é—¨ä¼˜åŒ–ä¸ºé–¢é›»å·¥ç½‘ç«™çˆ¬å–
"""

import os
import re
import time
import hashlib
import requests
import threading
from queue import Queue, Empty
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass
import json
from datetime import datetime

# HTML è§£æ
from bs4 import BeautifulSoup

# å¹¶å‘å¤„ç†
from concurrent.futures import ThreadPoolExecutor, as_completed

# å¢å¼ºç‰ˆå¤„ç†å™¨
from enhanced_html_to_milvus import EnhancedHTMLToMilvusProcessor

@dataclass
class EnhancedCrawlConfig:
    """å¢å¼ºç‰ˆçˆ¬è™«é…ç½®"""
    base_url: str
    max_pages: int = 500
    max_depth: int = 3
    concurrent_workers: int = 3
    delay_between_requests: float = 2.0
    timeout: int = 30
    collection_name: str = "kandenko_website_smart"
    quality_threshold: float = 0.4

class EnhancedWebsiteCrawler:
    """å¢å¼ºç‰ˆç½‘ç«™çˆ¬è™«ï¼Œé›†æˆæ™ºèƒ½åˆ†å—"""
    
    def __init__(self, config: EnhancedCrawlConfig):
        self.config = config
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()
        self.url_queue = Queue()
        self.results = []
        self.lock = threading.Lock()
        
        # è§£æåŸºç¡€åŸŸå
        parsed = urlparse(config.base_url)
        self.base_domain = parsed.netloc
        self.base_scheme = parsed.scheme
        
        # åˆå§‹åŒ–å¢å¼ºç‰ˆå¤„ç†å™¨
        self.processor = EnhancedHTMLToMilvusProcessor(
            collection_name=config.collection_name
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'pages_crawled': 0,
            'pages_failed': 0,
            'smart_chunks_created': 0,
            'high_quality_chunks': 0,
            'start_time': None,
            'end_time': None,
            'languages_detected': {'ja': 0, 'zh': 0, 'en': 0}
        }
        
        # è¯·æ±‚ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def _is_valid_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            return (
                parsed.netloc == self.base_domain and
                not any(exclude in url.lower() for exclude in [
                    'javascript:', 'mailto:', 'tel:', '.pdf', '.doc', '.zip',
                    'logout', 'login', 'admin', 'api/', '/search?'
                ])
            )
        except:
            return False
    
    def _fetch_page(self, url: str) -> Optional[Tuple[str, str]]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            print(f"ğŸŒ æ­£åœ¨è·å–: {url}")
            
            response = self.session.get(
                url, 
                timeout=self.config.timeout,
                allow_redirects=True
            )
            
            if response.status_code == 200:
                # å°è¯•æ£€æµ‹ç¼–ç 
                encoding = response.encoding
                if encoding.lower() in ['iso-8859-1', 'ascii']:
                    # æ£€æµ‹æ—¥æ–‡ç¼–ç 
                    import chardet
                    detected = chardet.detect(response.content)
                    if detected['encoding']:
                        encoding = detected['encoding']
                
                response.encoding = encoding
                content = response.text
                
                # æå–æ ‡é¢˜
                soup = BeautifulSoup(content, 'html.parser')
                title_tag = soup.find('title')
                title = title_tag.get_text().strip() if title_tag else ""
                
                print(f"âœ… æˆåŠŸè·å–: {url} (ç¼–ç : {encoding})")
                return content, title
            else:
                print(f"âŒ HTTPé”™è¯¯ {response.status_code}: {url}")
                return None
                
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥ {url}: {e}")
            return None
    
    def _extract_links(self, html_content: str, base_url: str) -> Set[str]:
        """æå–é¡µé¢ä¸­çš„é“¾æ¥"""
        links = set()
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            for link in soup.find_all('a', href=True):
                href = link['href'].strip()
                if href:
                    absolute_url = urljoin(base_url, href)
                    # ç§»é™¤ç‰‡æ®µæ ‡è¯†ç¬¦
                    absolute_url = absolute_url.split('#')[0]
                    
                    if self._is_valid_url(absolute_url):
                        links.add(absolute_url)
            
            print(f"ğŸ”— å‘ç° {len(links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
            return links
            
        except Exception as e:
            print(f"âŒ æå–é“¾æ¥å¤±è´¥: {e}")
            return set()
    
    def _crawl_page(self, url: str, depth: int) -> bool:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        if depth > self.config.max_depth:
            return False
        
        with self.lock:
            if url in self.visited_urls or len(self.visited_urls) >= self.config.max_pages:
                return False
            self.visited_urls.add(url)
        
        # è·å–é¡µé¢å†…å®¹
        result = self._fetch_page(url)
        if not result:
            with self.lock:
                self.failed_urls.add(url)
                self.stats['pages_failed'] += 1
            return False
        
        html_content, title = result
        
        # ä½¿ç”¨å¢å¼ºç‰ˆå¤„ç†å™¨è¿›è¡Œæ™ºèƒ½åˆ†å—
        try:
            items = self.processor.process_html_page(url, html_content, title)
            
            if items:
                # æ‰¹é‡ä¿å­˜
                if self.processor.save_items(items):
                    with self.lock:
                        self.stats['pages_crawled'] += 1
                        self.stats['smart_chunks_created'] += len(items)
                        
                        # ç»Ÿè®¡é«˜è´¨é‡å—
                        high_quality = sum(1 for item in items if item['quality_score'] >= self.config.quality_threshold)
                        self.stats['high_quality_chunks'] += high_quality
                        
                        # è¯­è¨€ç»Ÿè®¡
                        for item in items:
                            lang = item['language']
                            if lang in self.stats['languages_detected']:
                                self.stats['languages_detected'][lang] += 1
                
                print(f"âœ… é¡µé¢å¤„ç†å®Œæˆ: {url} ({len(items)} ä¸ªæ™ºèƒ½å—)")
            else:
                print(f"âš ï¸  é¡µé¢æ— æœ‰æ•ˆå†…å®¹: {url}")
        
        except Exception as e:
            print(f"âŒ å¤„ç†é¡µé¢å¤±è´¥ {url}: {e}")
            with self.lock:
                self.failed_urls.add(url)
                self.stats['pages_failed'] += 1
            return False
        
        # æå–æ–°é“¾æ¥ï¼ˆä»…åœ¨æµ…å±‚æ—¶ï¼‰
        if depth < self.config.max_depth - 1:
            new_links = self._extract_links(html_content, url)
            
            for link in new_links:
                if link not in self.visited_urls and link not in self.failed_urls:
                    self.url_queue.put((link, depth + 1))
        
        # è¯·æ±‚é—´å»¶æ—¶
        time.sleep(self.config.delay_between_requests)
        return True
    
    def crawl(self) -> Dict:
        """æ‰§è¡Œæ™ºèƒ½çˆ¬å–"""
        print(f"ğŸš€ å¼€å§‹æ™ºèƒ½çˆ¬å–: {self.config.base_url}")
        print(f"ğŸ“Š é…ç½®: æœ€å¤§é¡µé¢={self.config.max_pages}, æœ€å¤§æ·±åº¦={self.config.max_depth}")
        
        self.stats['start_time'] = time.time()
        
        # è¿æ¥åˆ°Milvus
        if not self.processor.connect():
            raise Exception("æ— æ³•è¿æ¥åˆ°Milvus")
        
        # åˆ›å»ºé›†åˆ
        if not self.processor.create_collection():
            raise Exception("æ— æ³•åˆ›å»ºMilvusé›†åˆ")
        
        # æ·»åŠ èµ·å§‹URL
        self.url_queue.put((self.config.base_url, 0))
        
        # å¤šçº¿ç¨‹çˆ¬å–
        with ThreadPoolExecutor(max_workers=self.config.concurrent_workers) as executor:
            futures = []
            
            while not self.url_queue.empty() or futures:
                # æäº¤æ–°ä»»åŠ¡
                while len(futures) < self.config.concurrent_workers and not self.url_queue.empty():
                    try:
                        url, depth = self.url_queue.get_nowait()
                        future = executor.submit(self._crawl_page, url, depth)
                        futures.append(future)
                    except Empty:
                        break
                
                # æ£€æŸ¥å®Œæˆçš„ä»»åŠ¡
                if futures:
                    completed_futures = []
                    for future in futures:
                        if future.done():
                            completed_futures.append(future)
                    
                    for future in completed_futures:
                        futures.remove(future)
                        try:
                            future.result()
                        except Exception as e:
                            print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                
                # çŸ­æš‚ç­‰å¾…
                time.sleep(0.1)
        
        self.stats['end_time'] = time.time()
        
        # åˆ›å»ºç´¢å¼•
        print("ğŸ”— åˆ›å»ºä¼˜åŒ–ç´¢å¼•...")
        self.processor.create_index()
        
        # å®Œæˆç»Ÿè®¡
        duration = self.stats['end_time'] - self.stats['start_time']
        success_rate = self.stats['pages_crawled'] / (self.stats['pages_crawled'] + self.stats['pages_failed']) if (self.stats['pages_crawled'] + self.stats['pages_failed']) > 0 else 0
        
        report = {
            'config': {
                'base_url': self.config.base_url,
                'collection_name': self.config.collection_name,
                'max_pages': self.config.max_pages
            },
            'stats': {
                'duration_seconds': duration,
                'pages_crawled': self.stats['pages_crawled'],
                'pages_failed': self.stats['pages_failed'],
                'smart_chunks_created': self.stats['smart_chunks_created'],
                'high_quality_chunks': self.stats['high_quality_chunks'],
                'success_rate': success_rate,
                'languages_detected': self.stats['languages_detected']
            },
            'processor_stats': self.processor.get_stats()
        }
        
        # æ‰“å°ç»Ÿè®¡
        self.print_final_stats()
        self.processor.print_stats()
        
        return report
    
    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ¯ æ™ºèƒ½çˆ¬å–æœ€ç»ˆç»Ÿè®¡:")
        print("=" * 60)
        duration = self.stats['end_time'] - self.stats['start_time']
        print(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f} ç§’")
        print(f"ğŸ“„ æˆåŠŸé¡µé¢: {self.stats['pages_crawled']}")
        print(f"âŒ å¤±è´¥é¡µé¢: {self.stats['pages_failed']}")
        print(f"ğŸ§© æ™ºèƒ½å—æ€»æ•°: {self.stats['smart_chunks_created']}")
        print(f"â­ é«˜è´¨é‡å—: {self.stats['high_quality_chunks']}")
        
        if self.stats['smart_chunks_created'] > 0:
            quality_rate = self.stats['high_quality_chunks'] / self.stats['smart_chunks_created']
            print(f"ğŸ“ˆ è´¨é‡ç‡: {quality_rate:.1%}")
        
        print(f"ğŸŒ è¯­è¨€åˆ†å¸ƒ:")
        for lang, count in self.stats['languages_detected'].items():
            if count > 0:
                lang_name = {'ja': 'æ—¥è¯­', 'zh': 'ä¸­æ–‡', 'en': 'è‹±è¯­'}.get(lang, lang)
                print(f"   {lang_name}: {count} å—")
        
        success_rate = self.stats['pages_crawled'] / (self.stats['pages_crawled'] + self.stats['pages_failed']) if (self.stats['pages_crawled'] + self.stats['pages_failed']) > 0 else 0
        print(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.1%}")
    
    def save_report(self) -> str:
        """ä¿å­˜çˆ¬å–æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"enhanced_crawl_report_{self.base_domain}_{timestamp}.json"
        
        report = {
            'config': self.config.__dict__,
            'stats': self.stats,
            'processor_stats': self.processor.get_stats(),
            'visited_urls': list(self.visited_urls),
            'failed_urls': list(self.failed_urls)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“‹ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename
    
    def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        self.processor.disconnect()

def create_kandenko_smart_config() -> EnhancedCrawlConfig:
    """åˆ›å»ºé–¢é›»å·¥æ™ºèƒ½çˆ¬å–é…ç½®"""
    return EnhancedCrawlConfig(
        base_url="https://www.kandenko.co.jp",
        max_pages=300,
        max_depth=3,
        concurrent_workers=2,
        delay_between_requests=2.0,
        collection_name="kandenko_website_smart",
        quality_threshold=0.5
    )

if __name__ == "__main__":
    config = create_kandenko_smart_config()
    crawler = EnhancedWebsiteCrawler(config)
    
    try:
        report = crawler.crawl()
        crawler.save_report()
        print("ğŸ‰ æ™ºèƒ½çˆ¬å–å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        crawler.disconnect()