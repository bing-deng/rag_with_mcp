#!/usr/bin/env python3
"""
åŸºäº LLaMA çš„æ™ºèƒ½æŸ¥è¯¢ç³»ç»Ÿ
ç»“åˆ Milvus å‘é‡æœç´¢å’Œ LLaMA å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç° RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) åŠŸèƒ½
"""

import json
import time
from typing import List, Dict, Optional
from query_milvus import MilvusQueryEngine

# å°è¯•å¯¼å…¥ä¸åŒçš„ LLaMA å®ç°
try:
    # ä¼˜å…ˆå°è¯• transformers åº“ï¼ˆé€‚ç”¨äº Hugging Face æ¨¡å‹ï¼‰
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    HAS_TRANSFORMERS = True
    print("âœ… å‘ç° transformers åº“ï¼Œæ”¯æŒ Hugging Face LLaMA æ¨¡å‹")
except ImportError:
    HAS_TRANSFORMERS = False

try:
    # å°è¯• llama-cpp-pythonï¼ˆé€‚ç”¨äºé‡åŒ–æ¨¡å‹ï¼‰
    from llama_cpp import Llama
    HAS_LLAMA_CPP = True
    print("âœ… å‘ç° llama-cpp-pythonï¼Œæ”¯æŒé‡åŒ– LLaMA æ¨¡å‹")
except ImportError:
    HAS_LLAMA_CPP = False

try:
    # å°è¯• Ollamaï¼ˆæœ¬åœ° LLaMA æœåŠ¡ï¼‰
    import requests
    HAS_OLLAMA = True
    print("âœ… æ”¯æŒ Ollama æœ¬åœ° LLaMA æœåŠ¡")
except ImportError:
    HAS_OLLAMA = False

class LLaMAQueryEngine:
    """ç»“åˆ LLaMA çš„æ™ºèƒ½æŸ¥è¯¢å¼•æ“"""
    
    def __init__(self, model_type='ollama', model_name='llama3.2:3b', collection_name='web_content', **kwargs):
        """
        åˆå§‹åŒ– LLaMA æŸ¥è¯¢å¼•æ“
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('ollama', 'transformers', 'llama_cpp')
            model_name: æ¨¡å‹åç§°
            collection_name: Milvus é›†åˆåç§°
            **kwargs: æ¨¡å‹ç‰¹å®šå‚æ•°
        """
        self.model_type = model_type
        self.model_name = model_name
        self.collection_name = collection_name
        self.model = None
        self.tokenizer = None
        
        # åˆå§‹åŒ– Milvus æŸ¥è¯¢å¼•æ“
        self.milvus_engine = MilvusQueryEngine(collection_name=collection_name)
        
        # åˆå§‹åŒ– LLaMA æ¨¡å‹
        self._initialize_model(**kwargs)
    
    def _initialize_model(self, **kwargs):
        """åˆå§‹åŒ– LLaMA æ¨¡å‹"""
        if self.model_type == 'ollama':
            self._initialize_ollama(**kwargs)
        elif self.model_type == 'transformers':
            self._initialize_transformers(**kwargs)
        elif self.model_type == 'llama_cpp':
            self._initialize_llama_cpp(**kwargs)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _initialize_ollama(self, base_url='http://localhost:11434', **kwargs):
        """åˆå§‹åŒ– Ollama æ¨¡å‹"""
        if not HAS_OLLAMA:
            print("âŒ éœ€è¦å®‰è£… requests åº“æ¥ä½¿ç”¨ Ollama")
            return
        
        self.ollama_base_url = base_url
        
        # æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦å¯ç”¨
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                available_models = [model['name'] for model in response.json().get('models', [])]
                print(f"âœ… Ollama æœåŠ¡å¯ç”¨ï¼Œå¯ç”¨æ¨¡å‹: {available_models}")
                
                if self.model_name not in available_models:
                    print(f"âš ï¸  æ¨¡å‹ {self.model_name} æœªå®‰è£…")
                    print(f"   å®‰è£…å‘½ä»¤: ollama pull {self.model_name}")
                else:
                    print(f"âœ… æ¨¡å‹ {self.model_name} å·²å°±ç»ª")
            else:
                print("âŒ Ollama æœåŠ¡å“åº”å¼‚å¸¸")
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡: {e}")
            print("   è¯·ç¡®ä¿ Ollama æœåŠ¡å·²å¯åŠ¨: ollama serve")
    
    def _initialize_transformers(self, device='auto', **kwargs):
        """åˆå§‹åŒ– Transformers æ¨¡å‹"""
        if not HAS_TRANSFORMERS:
            print("âŒ éœ€è¦å®‰è£… transformers å’Œ torch åº“")
            return
        
        try:
            print(f"ğŸ”§ åŠ è½½ Transformers æ¨¡å‹: {self.model_name}")
            
            # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
            if device == 'auto':
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
                device_map=device,
                **kwargs
            )
            
            print(f"âœ… Transformers æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {device})")
            
        except Exception as e:
            print(f"âŒ Transformers æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def _initialize_llama_cpp(self, model_path=None, **kwargs):
        """åˆå§‹åŒ– llama-cpp-python æ¨¡å‹"""
        if not HAS_LLAMA_CPP:
            print("âŒ éœ€è¦å®‰è£… llama-cpp-python åº“")
            return
        
        if not model_path:
            print("âŒ llama-cpp éœ€è¦æŒ‡å®š model_path å‚æ•°")
            return
        
        try:
            print(f"ğŸ”§ åŠ è½½ llama-cpp æ¨¡å‹: {model_path}")
            
            self.model = Llama(
                model_path=model_path,
                n_ctx=2048,  # ä¸Šä¸‹æ–‡é•¿åº¦
                n_threads=4,  # çº¿ç¨‹æ•°
                **kwargs
            )
            
            print("âœ… llama-cpp æ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ llama-cpp æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def connect_to_milvus(self):
        """è¿æ¥åˆ° Milvus"""
        return self.milvus_engine.connect()
    
    def _generate_with_ollama(self, prompt: str, max_tokens: int = 500) -> str:
        """ä½¿ç”¨ Ollama ç”Ÿæˆå›ç­”"""
        try:
            data = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.7
                }
            }
            
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                return f"âŒ Ollama è¯·æ±‚å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            return f"âŒ Ollama ç”Ÿæˆå¤±è´¥: {e}"
    
    def _generate_with_transformers(self, prompt: str, max_tokens: int = 500) -> str:
        """ä½¿ç”¨ Transformers ç”Ÿæˆå›ç­”"""
        try:
            inputs = self.tokenizer.encode(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # ç§»é™¤åŸå§‹ promptï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
            response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            return f"âŒ Transformers ç”Ÿæˆå¤±è´¥: {e}"
    
    def _generate_with_llama_cpp(self, prompt: str, max_tokens: int = 500) -> str:
        """ä½¿ç”¨ llama-cpp ç”Ÿæˆå›ç­”"""
        try:
            response = self.model(
                prompt,
                max_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                stop=["Human:", "Assistant:", "\n\n"]
            )
            
            return response['choices'][0]['text'].strip()
            
        except Exception as e:
            return f"âŒ llama-cpp ç”Ÿæˆå¤±è´¥: {e}"
    
    def generate_response(self, prompt: str, max_tokens: int = 500) -> str:
        """ç”Ÿæˆå›ç­”"""
        if self.model_type == 'ollama':
            return self._generate_with_ollama(prompt, max_tokens)
        elif self.model_type == 'transformers':
            return self._generate_with_transformers(prompt, max_tokens)
        elif self.model_type == 'llama_cpp':
            return self._generate_with_llama_cpp(prompt, max_tokens)
        else:
            return "âŒ æ¨¡å‹æœªåˆå§‹åŒ–"
    
    def rag_query(self, question: str, top_k: int = 5, max_tokens: int = 500) -> Dict:
        """RAG æŸ¥è¯¢ï¼šæ£€ç´¢+ç”Ÿæˆ"""
        print(f"ğŸ¤– RAG æŸ¥è¯¢: '{question}'")
        
        # 1. ä» Milvus æ£€ç´¢ç›¸å…³å†…å®¹
        print("ğŸ” ç¬¬ä¸€æ­¥ï¼šå‘é‡æœç´¢æ£€ç´¢ç›¸å…³å†…å®¹...")
        search_results = self.milvus_engine.basic_search(question, top_k=top_k)
        
        if not search_results:
            return {
                "question": question,
                "retrieved_contexts": [],
                "generated_answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                "sources": []
            }
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        contexts = []
        sources = []
        for i, result in enumerate(search_results):
            context = f"æ–‡æ¡£{i+1}: {result['content']}"
            contexts.append(context)
            sources.append({
                "id": result['id'],
                "title": result['title'],
                "url": result['url'],
                "content_type": result['content_type'],
                "similarity": result['score']
            })
        
        # 3. æ„å»º prompt
        context_text = "\n\n".join(contexts)
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚

èƒŒæ™¯ä¿¡æ¯ï¼š
{context_text}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ï¼š"""

        # 4. ç”Ÿæˆå›ç­”
        print("ğŸ¤– ç¬¬äºŒæ­¥ï¼šLLaMA ç”Ÿæˆæ™ºèƒ½å›ç­”...")
        generated_answer = self.generate_response(prompt, max_tokens)
        
        return {
            "question": question,
            "retrieved_contexts": contexts,
            "generated_answer": generated_answer,
            "sources": sources,
            "model_info": {
                "type": self.model_type,
                "name": self.model_name
            }
        }
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©æ¨¡å¼"""
        print("ğŸ¤– LLaMA + Milvus æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
        print("=" * 50)
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("è¾“å…¥ 'stats' æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡")
        print("-" * 50)
        
        if not self.connect_to_milvus():
            print("âŒ æ— æ³•è¿æ¥åˆ° Milvusï¼Œé€€å‡ºèŠå¤©æ¨¡å¼")
            return
        
        chat_history = []
        
        try:
            while True:
                question = input("\nğŸ™‹ æ‚¨çš„é—®é¢˜: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    break
                elif question.lower() == 'stats':
                    stats = self.milvus_engine.get_statistics()
                    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
                    print(f"   æ€»è®°å½•æ•°: {stats.get('total_records', 'N/A')}")
                    if 'content_type_distribution' in stats:
                        print("   å†…å®¹ç±»å‹åˆ†å¸ƒ:")
                        for ct, count in list(stats['content_type_distribution'].items())[:5]:
                            print(f"     {ct}: {count}")
                    continue
                elif not question:
                    continue
                
                # æ‰§è¡Œ RAG æŸ¥è¯¢
                start_time = time.time()
                result = self.rag_query(question, top_k=3, max_tokens=500)
                end_time = time.time()
                
                # æ˜¾ç¤ºç»“æœ
                print(f"\nğŸ¤– AI å›ç­”:")
                print("-" * 40)
                print(result['generated_answer'])
                
                print(f"\nğŸ“š å‚è€ƒæ¥æº (ç›¸ä¼¼åº¦):")
                for i, source in enumerate(result['sources'], 1):
                    print(f"  {i}. [{source['content_type']}] {source['title']} ({source['similarity']:.3f})")
                
                print(f"\nâ±ï¸  å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
                
                # ä¿å­˜èŠå¤©å†å²
                chat_history.append({
                    "question": question,
                    "answer": result['generated_answer'],
                    "timestamp": time.time()
                })
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
        
        finally:
            self.milvus_engine.disconnect()
            
            # ä¿å­˜èŠå¤©å†å²
            if chat_history:
                filename = f"chat_history_{int(time.time())}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(chat_history, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ èŠå¤©å†å²å·²ä¿å­˜åˆ°: {filename}")

def setup_ollama_guide():
    """Ollama å®‰è£…å’Œè®¾ç½®æŒ‡å—"""
    print("ğŸ“– Ollama è®¾ç½®æŒ‡å—")
    print("=" * 40)
    print("1. å®‰è£… Ollama:")
    print("   macOS: brew install ollama")
    print("   Linux: curl -fsSL https://ollama.ai/install.sh | sh")
    print("   Windows: ä¸‹è½½ https://ollama.ai/download/windows")
    print()
    print("2. å¯åŠ¨ Ollama æœåŠ¡:")
    print("   ollama serve")
    print()
    print("3. ä¸‹è½½ LLaMA æ¨¡å‹:")
    print("   ollama pull llama3.2:3b    # 3B æ¨¡å‹ (æ¨è)")
    print("   ollama pull llama2         # åŸºç¡€æ¨¡å‹")
    print("   ollama pull llama2:13b     # 13B æ¨¡å‹")
    print("   ollama pull codellama      # ä»£ç ä¸“ç”¨")
    print()
    print("4. æµ‹è¯•æ¨¡å‹:")
    print("   ollama run llama3.2:3b")

def demo_different_models():
    """æ¼”ç¤ºä¸åŒ LLaMA å®ç°çš„ä½¿ç”¨"""
    print("ğŸ­ ä¸åŒ LLaMA å®ç°æ¼”ç¤º")
    print("=" * 50)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_question = "ä»€ä¹ˆæ˜¯é‡‘èç›‘ç®¡ï¼Ÿ"
    
    # 1. Ollama æ¼”ç¤º
    print("\n1ï¸âƒ£  Ollama æ¼”ç¤º:")
    try:
        ollama_engine = LLaMAQueryEngine(model_type='ollama', model_name='llama3.2:3b', collection_name='kandenko_website')
        if ollama_engine.connect_to_milvus():
            result = ollama_engine.rag_query(test_question, top_k=2)
            print(f"é—®é¢˜: {result['question']}")
            print(f"å›ç­”: {result['generated_answer'][:200]}...")
            ollama_engine.milvus_engine.disconnect()
    except Exception as e:
        print(f"âŒ Ollama æ¼”ç¤ºå¤±è´¥: {e}")
    
    # 2. Transformers æ¼”ç¤º (éœ€è¦å¤§é‡å†…å­˜)
    print("\n2ï¸âƒ£  Transformers æ¼”ç¤º:")
    print("âš ï¸  éœ€è¦å¤§é‡ GPU å†…å­˜ï¼Œè·³è¿‡æ¼”ç¤º")
    print("   å¦‚éœ€ä½¿ç”¨ï¼Œè¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU èµ„æº")
    
    # 3. llama-cpp æ¼”ç¤º
    print("\n3ï¸âƒ£  llama-cpp æ¼”ç¤º:")
    print("âš ï¸  éœ€è¦ä¸‹è½½é‡åŒ–æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡æ¼”ç¤º")
    print("   å¦‚éœ€ä½¿ç”¨ï¼Œè¯·ä¸‹è½½ .gguf æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– LLaMA + Milvus æ™ºèƒ½æŸ¥è¯¢ç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥å¯ç”¨çš„å®ç°
    available_implementations = []
    if HAS_OLLAMA:
        available_implementations.append("Ollama (æ¨è)")
    if HAS_TRANSFORMERS:
        available_implementations.append("Transformers")
    if HAS_LLAMA_CPP:
        available_implementations.append("llama-cpp-python")
    
    if not available_implementations:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ LLaMA å®ç°")
        print("\næ¨èå®‰è£… Ollama:")
        setup_ollama_guide()
        return
    
    print(f"âœ… å¯ç”¨çš„å®ç°: {', '.join(available_implementations)}")
    
    # é€‰æ‹©å®ç°æ–¹å¼
    print("\nè¯·é€‰æ‹©ä½¿ç”¨æ–¹å¼:")
    print("1. äº¤äº’å¼é—®ç­” (Ollama)")
    print("2. æŸ¥çœ‹ Ollama è®¾ç½®æŒ‡å—") 
    print("3. æ¼”ç¤ºä¸åŒå®ç°")
    print("0. é€€å‡º")
    
    choice = input("\nè¯·é€‰æ‹© (0-3): ").strip()
    
    if choice == "1":
        # äº¤äº’å¼é—®ç­”
        engine = LLaMAQueryEngine(model_type='ollama', model_name='llama3.2:3b', collection_name='kandenko_website')
        engine.interactive_chat()
    
    elif choice == "2":
        # è®¾ç½®æŒ‡å—
        setup_ollama_guide()
    
    elif choice == "3":
        # æ¼”ç¤ºä¸åŒå®ç°
        demo_different_models()
    
    elif choice == "0":
        print("ğŸ‘‹ å†è§ï¼")
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main() 