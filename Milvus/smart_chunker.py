#!/usr/bin/env python3
"""
æ™ºèƒ½å†…å®¹åˆ†å—å™¨
è§£å†³å½“å‰å†…å®¹åˆ†å‰²è¿‡äºç»†ç²’åº¦çš„é—®é¢˜ï¼Œæå‡RAGæ£€ç´¢è´¨é‡
"""

import re
import numpy as np
from typing import List, Dict, Tuple
from dataclasses import dataclass
import nltk
from nltk.tokenize import sent_tokenize

# ç¡®ä¿NLTKæ•°æ®å¯ç”¨
try:
    sent_tokenize("test")
except LookupError:
    import ssl
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context
    nltk.download('punkt')

@dataclass
class ChunkMetadata:
    """åˆ†å—å…ƒæ•°æ®"""
    original_url: str
    chunk_index: int
    total_chunks: int
    content_type: str
    quality_score: float
    language: str
    word_count: int

class SmartChunker:
    """æ™ºèƒ½å†…å®¹åˆ†å—å™¨"""
    
    def __init__(self, 
                 min_chunk_size: int = 150,
                 max_chunk_size: int = 800, 
                 overlap_size: int = 50,
                 target_chunk_size: int = 400):
        """
        Args:
            min_chunk_size: æœ€å°å—å¤§å°ï¼ˆå­—ç¬¦ï¼‰
            max_chunk_size: æœ€å¤§å—å¤§å°ï¼ˆå­—ç¬¦ï¼‰
            overlap_size: é‡å å¤§å°ï¼ˆå­—ç¬¦ï¼‰
            target_chunk_size: ç›®æ ‡å—å¤§å°ï¼ˆå­—ç¬¦ï¼‰
        """
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size
        self.target_chunk_size = target_chunk_size
        
    def chunk_content(self, content: str, metadata: Dict = None) -> List[Tuple[str, ChunkMetadata]]:
        """
        æ™ºèƒ½åˆ†å—å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            metadata: å†…å®¹å…ƒæ•°æ®
            
        Returns:
            åˆ†å—åçš„å†…å®¹å’Œå…ƒæ•°æ®åˆ—è¡¨
        """
        if not content or len(content) < self.min_chunk_size:
            return []
        
        # 1. é¢„å¤„ç†ï¼šæ¸…ç†å’Œæ ‡å‡†åŒ–
        cleaned_content = self._preprocess_content(content)
        
        # 2. è´¨é‡è¯„ä¼°
        quality_score = self._assess_content_quality(cleaned_content)
        if quality_score < 0.3:  # è´¨é‡å¤ªä½ï¼Œè·³è¿‡
            return []
        
        # 3. è¯­è¨€æ£€æµ‹
        language = self._detect_language(cleaned_content)
        
        # 4. é€‰æ‹©åˆ†å—ç­–ç•¥
        if language == 'ja':
            chunks = self._chunk_japanese_content(cleaned_content)
        elif language in ['zh', 'zh-cn']:
            chunks = self._chunk_chinese_content(cleaned_content)
        else:
            chunks = self._chunk_english_content(cleaned_content)
        
        # 5. åå¤„ç†ï¼šé‡å å’Œè´¨é‡è¿‡æ»¤
        processed_chunks = self._post_process_chunks(chunks)
        
        # 6. ç”Ÿæˆå…ƒæ•°æ®
        result = []
        for i, chunk in enumerate(processed_chunks):
            chunk_metadata = ChunkMetadata(
                original_url=metadata.get('url', '') if metadata else '',
                chunk_index=i,
                total_chunks=len(processed_chunks),
                content_type=metadata.get('content_type', 'text') if metadata else 'text',
                quality_score=self._assess_content_quality(chunk),
                language=language,
                word_count=len(chunk.split())
            )
            result.append((chunk, chunk_metadata))
        
        return result
    
    def _preprocess_content(self, content: str) -> str:
        """é¢„å¤„ç†å†…å®¹"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content.strip())
        
        # ç§»é™¤å¸¸è§çš„å™ªå£°æ¨¡å¼
        noise_patterns = [
            r'^(é¦–é¡µ|ä¸»é¡µ|ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸|Home)$',
            r'^(å¯¼èˆª|ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³|Navigation).*',
            r'^(èœå•|ãƒ¡ãƒ‹ãƒ¥ãƒ¼|Menu).*',
            r'^(é¡µè„š|ãƒ•ãƒƒã‚¿ãƒ¼|Footer).*',
            r'^(ç‰ˆæƒ|è‘—ä½œæ¨©|Copyright).*',
            r'^(è”ç³»æˆ‘ä»¬|ãŠå•ã„åˆã‚ã›|Contact)$',
        ]
        
        for pattern in noise_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.MULTILINE)
        
        return content.strip()
    
    def _assess_content_quality(self, content: str) -> float:
        """è¯„ä¼°å†…å®¹è´¨é‡ (0-1åˆ†æ•°)"""
        if not content:
            return 0.0
        
        score = 0.0
        
        # 1. é•¿åº¦æƒé‡ (30%)
        length = len(content)
        if length < 50:
            length_score = 0.0
        elif length < 100:
            length_score = 0.3
        elif length < 200:
            length_score = 0.7
        else:
            length_score = 1.0
        score += length_score * 0.3
        
        # 2. ä¿¡æ¯å¯†åº¦ (40%) - éé‡å¤ã€æœ‰æ„ä¹‰çš„å†…å®¹æ¯”ä¾‹
        words = content.split()
        if len(words) == 0:
            return 0.0
        
        unique_words = set(words)
        info_density = len(unique_words) / len(words)
        score += min(info_density * 2, 1.0) * 0.4
        
        # 3. è¯­è¨€å®Œæ•´æ€§ (30%) - å®Œæ•´å¥å­æ¯”ä¾‹
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
        complete_sentences = [s for s in sentences if len(s.strip().split()) >= 3]
        if len(sentences) > 0:
            completeness = len(complete_sentences) / len(sentences)
        else:
            completeness = 0.0
        score += completeness * 0.3
        
        return min(score, 1.0)
    
    def _detect_language(self, content: str) -> str:
        """ç®€å•çš„è¯­è¨€æ£€æµ‹"""
        # æ—¥æ–‡æ£€æµ‹ï¼ˆå¹³å‡åã€ç‰‡å‡åï¼‰
        if re.search(r'[\u3040-\u309F\u30A0-\u30FF]', content):
            return 'ja'
        
        # ä¸­æ–‡æ£€æµ‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        if chinese_chars > len(content) * 0.3:
            return 'zh'
        
        # é»˜è®¤è‹±æ–‡
        return 'en'
    
    def _chunk_japanese_content(self, content: str) -> List[str]:
        """æ—¥æ–‡å†…å®¹åˆ†å—"""
        # æ—¥æ–‡å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        sentences = [s.strip() + 'ã€‚' for s in sentences if s.strip()]
        
        return self._merge_sentences_by_size(sentences)
    
    def _chunk_chinese_content(self, content: str) -> List[str]:
        """ä¸­æ–‡å†…å®¹åˆ†å—"""
        # ä¸­æ–‡å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', content)
        sentences = [s.strip() + 'ã€‚' for s in sentences if s.strip()]
        
        return self._merge_sentences_by_size(sentences)
    
    def _chunk_english_content(self, content: str) -> List[str]:
        """è‹±æ–‡å†…å®¹åˆ†å—"""
        try:
            sentences = sent_tokenize(content)
        except:
            # fallbackåˆ†å‰²
            sentences = re.split(r'[.!?]+\s+', content)
            sentences = [s.strip() + '.' for s in sentences if s.strip()]
        
        return self._merge_sentences_by_size(sentences)
    
    def _merge_sentences_by_size(self, sentences: List[str]) -> List[str]:
        """æ ¹æ®å¤§å°åˆå¹¶å¥å­"""
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°å¥å­ä¸è¶…è¿‡æœ€å¤§å¤§å°
            if len(current_chunk + sentence) <= self.max_chunk_size:
                current_chunk += sentence + " "
            else:
                # å¦‚æœå½“å‰å—è¾¾åˆ°æœ€å°å¤§å°ï¼Œä¿å­˜å®ƒ
                if len(current_chunk) >= self.min_chunk_size:
                    chunks.append(current_chunk.strip())
                
                # å¼€å§‹æ–°å—
                current_chunk = sentence + " "
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if len(current_chunk) >= self.min_chunk_size:
            chunks.append(current_chunk.strip())
        elif chunks and len(current_chunk) > 0:
            # å¦‚æœæœ€åä¸€å—å¤ªå°ï¼Œåˆå¹¶åˆ°å‰ä¸€å—
            chunks[-1] += " " + current_chunk.strip()
        
        return chunks
    
    def _post_process_chunks(self, chunks: List[str]) -> List[str]:
        """åå¤„ç†åˆ†å—ï¼šæ·»åŠ é‡å ï¼Œè´¨é‡è¿‡æ»¤"""
        if not chunks:
            return []
        
        processed = []
        
        for i, chunk in enumerate(chunks):
            # è´¨é‡è¿‡æ»¤
            if self._assess_content_quality(chunk) < 0.4:
                continue
            
            # æ·»åŠ é‡å ï¼ˆä¸å‰ä¸€å—ï¼‰ï¼Œä½†ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
            if i > 0 and self.overlap_size > 0:
                prev_chunk = chunks[i-1]
                overlap_text = prev_chunk[-self.overlap_size:]
                potential_chunk = overlap_text + " " + chunk
                
                # å¦‚æœåŠ ä¸Šé‡å åè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ™æˆªæ–­
                if len(potential_chunk) > self.max_chunk_size:
                    available_space = self.max_chunk_size - len(chunk) - 1  # -1 for space
                    if available_space > 0:
                        overlap_text = overlap_text[:available_space]
                        chunk = overlap_text + " " + chunk
                    # å¦‚æœæ²¡æœ‰ç©ºé—´ï¼Œå°±ä¸æ·»åŠ é‡å 
                else:
                    chunk = potential_chunk
            
            # æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ä¸è¶…è¿‡ç»å¯¹æœ€å¤§é•¿åº¦
            if len(chunk) > 1900:  # ç•™100å­—ç¬¦ç»å¯¹å®‰å…¨ç¼“å†²
                chunk = chunk[:1900] + "..."
                print(f"âš ï¸  å—è¿‡é•¿ï¼Œå¼ºåˆ¶æˆªæ–­åˆ°1900å­—ç¬¦")
            
            processed.append(chunk)
        
        return processed

def test_smart_chunker():
    """æµ‹è¯•æ™ºèƒ½åˆ†å—å™¨"""
    chunker = SmartChunker()
    
    # æµ‹è¯•æ—¥æ–‡å†…å®¹
    japanese_text = """
    æ ªå¼ä¼šç¤¾é–¢é›»å·¥ã¯ã€æ—¥æœ¬ã®é›»æ°—å·¥äº‹æ¥­ç•Œã‚’ãƒªãƒ¼ãƒ‰ã™ã‚‹ä¼æ¥­ã§ã™ã€‚
    å½“ç¤¾ã¯1944å¹´ã«è¨­ç«‹ã•ã‚Œã€é›»åŠ›ã‚¤ãƒ³ãƒ•ãƒ©ã®æ§‹ç¯‰ã¨ä¿å®ˆã«é•·å¹´å¾“äº‹ã—ã¦ãã¾ã—ãŸã€‚
    ä¸»è¦äº‹æ¥­ã«ã¯é€é…é›»å·¥äº‹ã€é›»æ°—è¨­å‚™å·¥äº‹ã€æƒ…å ±é€šä¿¡å·¥äº‹ãªã©ãŒã‚ã‚Šã¾ã™ã€‚
    é–¢é›»å·¥ã¯æŠ€è¡“é©æ–°ã‚’é‡è¦–ã—ã€æŒç¶šå¯èƒ½ãªç¤¾ä¼šã®å®Ÿç¾ã«å‘ã‘ã¦å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚
    """
    
    chunks = chunker.chunk_content(japanese_text, {'url': 'test.com', 'content_type': 'paragraph'})
    
    print("ğŸ§ª æ™ºèƒ½åˆ†å—å™¨æµ‹è¯•ç»“æœ")
    print("=" * 50)
    print(f"åŸæ–‡é•¿åº¦: {len(japanese_text)} å­—ç¬¦")
    print(f"åˆ†å—æ•°é‡: {len(chunks)}")
    print()
    
    for i, (chunk, metadata) in enumerate(chunks):
        print(f"å— {i+1}:")
        print(f"  å†…å®¹: {chunk[:100]}...")
        print(f"  é•¿åº¦: {len(chunk)} å­—ç¬¦")
        print(f"  è´¨é‡åˆ†æ•°: {metadata.quality_score:.2f}")
        print(f"  è¯­è¨€: {metadata.language}")
        print()

if __name__ == "__main__":
    test_smart_chunker()