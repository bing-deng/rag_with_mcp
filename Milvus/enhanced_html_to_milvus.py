#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆ HTML å†…å®¹å‘é‡åŒ–å­˜å‚¨ç³»ç»Ÿ
é›†æˆæ™ºèƒ½åˆ†å—å¤„ç†ï¼Œæå‡å†…å®¹è´¨é‡
"""

import os
import re
import time
import hashlib
import requests
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Tuple, Optional
import numpy as np

# HTML è§£æç›¸å…³
from bs4 import BeautifulSoup

# Milvus ç›¸å…³
from pymilvus import (
    connections, utility, FieldSchema, CollectionSchema, 
    DataType, Collection, MilvusException
)

# æ™ºèƒ½åˆ†å—å¤„ç†
from smart_chunker import SmartChunker, ChunkMetadata

# å‘é‡åŒ–ç›¸å…³
try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
    print("âœ… ä½¿ç”¨ sentence-transformers è¿›è¡Œè¯­ä¹‰å‘é‡åŒ–")
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("âš ï¸  sentence-transformers æœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–å‘é‡åŒ–")

class EnhancedHTMLToMilvusProcessor:
    """å¢å¼ºç‰ˆ HTML å†…å®¹åˆ° Milvus çš„å¤„ç†å™¨ï¼Œé›†æˆæ™ºèƒ½åˆ†å—"""
    
    def __init__(self, host='localhost', port='19530', collection_name='enhanced_content'):
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.collection = None
        self.dimension = 384
        
        # åˆå§‹åŒ–æ™ºèƒ½åˆ†å—å™¨ - ä¸¥æ ¼æ§åˆ¶åœ¨Milvuså­—æ®µé™åˆ¶å†…
        self.chunker = SmartChunker(
            min_chunk_size=150,  # æœ€å°150å­—ç¬¦
            max_chunk_size=1500, # æœ€å¤§1500å­—ç¬¦ï¼ˆç•™500å­—ç¬¦å®‰å…¨ç¼“å†²ï¼‰
            overlap_size=30,     # å‡å°‘é‡å åˆ°30å­—ç¬¦
            target_chunk_size=600 # ç›®æ ‡600å­—ç¬¦
        )
        
        # åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹
        if HAS_SENTENCE_TRANSFORMERS:
            print("ğŸ”§ åŠ è½½å¢å¼ºç‰ˆè¯­ä¹‰å‘é‡åŒ–æ¨¡å‹...")
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… å¢å¼ºç‰ˆè¯­ä¹‰æ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            self.model = None
            print("âš ï¸  å°†ä½¿ç”¨ç®€åŒ–å‘é‡åŒ–")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_pages_processed': 0,
            'total_chunks_created': 0,
            'high_quality_chunks': 0,
            'filtered_low_quality': 0,
            'multilingual_detection': {'ja': 0, 'zh': 0, 'en': 0}
        }
    
    def connect(self) -> bool:
        """è¿æ¥åˆ° Milvus"""
        try:
            connections.connect("default", host=self.host, port=self.port)
            print(f"âœ… æˆåŠŸè¿æ¥åˆ° Milvus: {self.host}:{self.port}")
            return True
        except Exception as e:
            print(f"âŒ è¿æ¥ Milvus å¤±è´¥: {e}")
            return False
    
    def create_collection(self) -> bool:
        """åˆ›å»ºå¢å¼ºç‰ˆé›†åˆ"""
        try:
            # åˆ é™¤ç°æœ‰é›†åˆ
            if utility.has_collection(self.collection_name):
                utility.drop_collection(self.collection_name)
                print(f"ğŸ—‘ï¸  åˆ é™¤ç°æœ‰é›†åˆ: {self.collection_name}")
            
            # å®šä¹‰å¢å¼ºç‰ˆå­—æ®µ
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=8000),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dimension),
                FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=200),
                FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=50),
                FieldSchema(name="chunk_index", dtype=DataType.INT32),
                FieldSchema(name="total_chunks", dtype=DataType.INT32),
                FieldSchema(name="quality_score", dtype=DataType.FLOAT),
                FieldSchema(name="language", dtype=DataType.VARCHAR, max_length=10),
                FieldSchema(name="word_count", dtype=DataType.INT32),
                FieldSchema(name="timestamp", dtype=DataType.INT64)
            ]
            
            schema = CollectionSchema(fields, "Enhanced website content with smart chunking")
            self.collection = Collection(self.collection_name, schema)
            
            print(f"âœ… åˆ›å»ºå¢å¼ºç‰ˆé›†åˆ: {self.collection_name}")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            return False
    
    def create_index(self):
        """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
        try:
            # å‘é‡ç´¢å¼•
            index_params = {
                "metric_type": "COSINE",
                "index_type": "HNSW",
                "params": {"M": 48, "efConstruction": 500}
            }
            
            self.collection.create_index("embedding", index_params)
            
            # æ ‡é‡å­—æ®µç´¢å¼•
            self.collection.create_index("language")
            self.collection.create_index("content_type") 
            self.collection.create_index("quality_score")
            
            print("âœ… åˆ›å»ºä¼˜åŒ–ç´¢å¼•å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
    
    def text_to_vector(self, text: str) -> List[float]:
        """æ–‡æœ¬è½¬å‘é‡"""
        if self.model:
            try:
                embedding = self.model.encode(text, normalize_embeddings=True)
                return embedding.tolist()
            except Exception as e:
                print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
                return None
        else:
            # ç®€åŒ–ç‰ˆå‘é‡åŒ–
            return self._simple_text_vector(text)
    
    def _simple_text_vector(self, text: str) -> List[float]:
        """ç®€åŒ–ç‰ˆæ–‡æœ¬å‘é‡åŒ–"""
        # åŸºäºå­—ç¬¦ç»Ÿè®¡çš„ç®€å•å‘é‡åŒ–
        vector = [0.0] * self.dimension
        
        if not text:
            return vector
        
        # åŸºæœ¬ç‰¹å¾
        vector[0] = len(text) / 1000.0  # é•¿åº¦ç‰¹å¾
        vector[1] = len(text.split()) / 100.0  # è¯æ•°ç‰¹å¾
        
        # å­—ç¬¦é¢‘ç‡ç‰¹å¾
        char_counts = {}
        for char in text:
            char_counts[char] = char_counts.get(char, 0) + 1
        
        # å¡«å……å‘é‡çš„å…¶ä½™éƒ¨åˆ†
        for i, (char, count) in enumerate(sorted(char_counts.items())[:self.dimension-2]):
            if i + 2 < self.dimension:
                vector[i + 2] = count / len(text)
        
        return vector
    
    def process_html_page(self, url: str, html_content: str, title: str = "") -> List[Dict]:
        """å¤„ç†HTMLé¡µé¢ï¼Œè¿”å›æ™ºèƒ½åˆ†å—çš„å†…å®¹"""
        print(f"ğŸ”„ æ™ºèƒ½å¤„ç†é¡µé¢: {url}")
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–å’Œåˆå¹¶æ–‡æœ¬å†…å®¹
            page_content = self._extract_page_content(soup)
            
            if not page_content:
                print(f"âš ï¸  é¡µé¢æ— æœ‰æ•ˆå†…å®¹: {url}")
                return []
            
            # ä½¿ç”¨æ™ºèƒ½åˆ†å—å™¨å¤„ç†å†…å®¹
            metadata = {
                'url': url,
                'title': title,
                'content_type': 'enhanced_page_content'
            }
            
            chunks = self.chunker.chunk_content(page_content, metadata)
            
            if not chunks:
                print(f"âš ï¸  æ™ºèƒ½åˆ†å—æ— ç»“æœ: {url}")
                self.stats['filtered_low_quality'] += 1
                return []
            
            # å‡†å¤‡å­˜å‚¨çš„æ•°æ®
            processed_items = []
            current_time = int(time.time())
            
            for chunk_text, chunk_metadata in chunks:
                # å‘é‡åŒ–
                vector = self.text_to_vector(chunk_text)
                if not vector:
                    continue
                
                # è´¨é‡è¯„ä¼°
                if chunk_metadata.quality_score >= 0.4:  # é«˜è´¨é‡é˜ˆå€¼
                    self.stats['high_quality_chunks'] += 1
                else:
                    self.stats['filtered_low_quality'] += 1
                    continue  # è·³è¿‡ä½è´¨é‡å—
                
                # è¯­è¨€ç»Ÿè®¡
                lang = chunk_metadata.language
                if lang in self.stats['multilingual_detection']:
                    self.stats['multilingual_detection'][lang] += 1
                
                item = {
                    'content': chunk_text,
                    'embedding': vector,
                    'url': url,
                    'title': title,
                    'content_type': 'smart_chunk',
                    'chunk_index': chunk_metadata.chunk_index,
                    'total_chunks': chunk_metadata.total_chunks,
                    'quality_score': chunk_metadata.quality_score,
                    'language': chunk_metadata.language,
                    'word_count': chunk_metadata.word_count,
                    'timestamp': current_time
                }
                
                processed_items.append(item)
            
            self.stats['total_pages_processed'] += 1
            self.stats['total_chunks_created'] += len(processed_items)
            
            print(f"âœ… æ™ºèƒ½åˆ†å—å®Œæˆ: {len(processed_items)} ä¸ªé«˜è´¨é‡å—")
            return processed_items
            
        except Exception as e:
            print(f"âŒ å¤„ç†é¡µé¢å¤±è´¥ {url}: {e}")
            return []
    
    def _extract_page_content(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢çš„ä¸»è¦æ–‡æœ¬å†…å®¹"""
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # ä¼˜å…ˆæå–ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = ""
        
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        main_selectors = [
            'main', 'article', '.content', '.main-content', 
            '#content', '#main', '.post-content', '.entry-content'
        ]
        
        for selector in main_selectors:
            elements = soup.select(selector)
            if elements:
                main_content = ' '.join([elem.get_text().strip() for elem in elements])
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦åŒºåŸŸï¼Œæå–bodyå†…å®¹
        if not main_content:
            body = soup.find('body')
            if body:
                main_content = body.get_text()
        
        # æ¸…ç†æ–‡æœ¬
        main_content = re.sub(r'\s+', ' ', main_content).strip()
        
        return main_content
    
    def save_items(self, items: List[Dict]) -> bool:
        """æ‰¹é‡ä¿å­˜å¤„ç†å¥½çš„é¡¹ç›®åˆ°Milvus"""
        if not items:
            return True
        
        try:
            # è¿‡æ»¤è¶…é•¿å†…å®¹å¹¶æˆªæ–­
            filtered_items = []
            for item in items:
                content = item['content']
                if len(content) > 7900:
                    print(f"âš ï¸  å†…å®¹è¿‡é•¿ï¼Œæˆªæ–­: {len(content)} -> 7900 å­—ç¬¦")
                    content = content[:7900] + "..."  # æˆªæ–­å¹¶æ·»åŠ çœç•¥å·
                    item = item.copy()
                    item['content'] = content
                filtered_items.append(item)
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
            entities = [
                [item['content'] for item in filtered_items],
                [item['embedding'] for item in filtered_items],
                [item['url'] for item in filtered_items],
                [item['title'] for item in filtered_items], 
                [item['content_type'] for item in filtered_items],
                [item['chunk_index'] for item in filtered_items],
                [item['total_chunks'] for item in filtered_items],
                [item['quality_score'] for item in filtered_items],
                [item['language'] for item in filtered_items],
                [item['word_count'] for item in filtered_items],
                [item['timestamp'] for item in filtered_items]
            ]
            
            # æ’å…¥åˆ°Milvus
            mr = self.collection.insert(entities)
            
            print(f"âœ… æˆåŠŸä¿å­˜ {len(filtered_items)} ä¸ªæ™ºèƒ½åˆ†å—åˆ°Milvus")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ°Milvuså¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š æ™ºèƒ½åˆ†å—å¤„ç†ç»Ÿè®¡:")
        print("=" * 50)
        print(f"ğŸ“„ å¤„ç†é¡µé¢æ•°: {self.stats['total_pages_processed']}")
        print(f"ğŸ§© ç”Ÿæˆå—æ•°: {self.stats['total_chunks_created']}")
        print(f"â­ é«˜è´¨é‡å—: {self.stats['high_quality_chunks']}")
        print(f"ğŸ—‘ï¸ è¿‡æ»¤ä½è´¨é‡: {self.stats['filtered_low_quality']}")
        print(f"ğŸŒ è¯­è¨€åˆ†å¸ƒ:")
        for lang, count in self.stats['multilingual_detection'].items():
            lang_name = {'ja': 'æ—¥è¯­', 'zh': 'ä¸­æ–‡', 'en': 'è‹±è¯­'}.get(lang, lang)
            print(f"   {lang_name}: {count} å—")
        
        if self.stats['total_chunks_created'] > 0:
            quality_rate = self.stats['high_quality_chunks'] / self.stats['total_chunks_created']
            print(f"ğŸ“ˆ è´¨é‡ç‡: {quality_rate:.1%}")
    
    def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        try:
            connections.disconnect("default")
            print("ğŸ”Œ å·²æ–­å¼€Milvusè¿æ¥")
        except:
            pass

def test_enhanced_processor():
    """æµ‹è¯•å¢å¼ºç‰ˆå¤„ç†å™¨"""
    processor = EnhancedHTMLToMilvusProcessor(collection_name="test_enhanced")
    
    if not processor.connect():
        return
    
    if not processor.create_collection():
        return
    
    # æµ‹è¯•HTMLå†…å®¹
    test_html = """
    <html>
    <body>
        <main>
            <h1>æ ªå¼ä¼šç¤¾é–¢é›»å·¥ã«ã¤ã„ã¦</h1>
            <p>æ ªå¼ä¼šç¤¾é–¢é›»å·¥ã¯ã€1944å¹´ã«è¨­ç«‹ã•ã‚ŒãŸæ—¥æœ¬ã®é›»æ°—å·¥äº‹ä¼šç¤¾ã§ã™ã€‚</p>
            <p>ä¸»ãªäº‹æ¥­å†…å®¹ã«ã¯ã€é›»åŠ›å·¥äº‹ã€é›»æ°—è¨­å‚™å·¥äº‹ã€æƒ…å ±é€šä¿¡å·¥äº‹ã€åœŸæœ¨å·¥äº‹ãªã©ãŒã‚ã‚Šã¾ã™ã€‚</p>
            <p>é–¢é›»å·¥ã¯ã€æŒç¶šå¯èƒ½ãªç¤¾ä¼šã®å®Ÿç¾ã«å‘ã‘ã¦ã€æŠ€è¡“é©æ–°ã¨ç’°å¢ƒä¿è­·ã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚</p>
        </main>
    </body>
    </html>
    """
    
    # å‡¦ç†
    items = processor.process_html_page("https://test.com", test_html, "ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸")
    
    if items:
        processor.save_items(items)
    
    processor.print_stats()
    processor.disconnect()

if __name__ == "__main__":
    test_enhanced_processor()