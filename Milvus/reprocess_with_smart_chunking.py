#!/usr/bin/env python3
"""
ä½¿ç”¨æ™ºèƒ½åˆ†å—é‡æ–°å¤„ç†ç°æœ‰æ•°æ®
å°†ç°æœ‰çš„ç»†ç²’åº¦æ•°æ®é‡æ–°ç»„ç»‡æˆæ›´æœ‰æ„ä¹‰çš„å—
"""

import os
import sys
import time
from typing import List, Dict, Tuple
from collections import defaultdict

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from pymilvus import connections, utility, Collection
from smart_chunker import SmartChunker, ChunkMetadata
from html_to_milvus import HTMLToMilvusProcessor

class DataReprocessor:
    """æ•°æ®é‡å¤„ç†å™¨"""
    
    def __init__(self, collection_name: str = "kandenko_website"):
        self.collection_name = collection_name
        self.backup_collection_name = f"{collection_name}_backup"
        self.new_collection_name = f"{collection_name}_smart"
        self.chunker = SmartChunker()
        self.processor = HTMLToMilvusProcessor(collection_name=self.new_collection_name)
        
    def backup_existing_data(self) -> bool:
        """å¤‡ä»½ç°æœ‰æ•°æ®"""
        try:
            print("ğŸ’¾ å¤‡ä»½ç°æœ‰æ•°æ®...")
            connections.connect("default", host="localhost", port="19530")
            
            # å¦‚æœå¤‡ä»½å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if utility.has_collection(self.backup_collection_name):
                utility.drop_collection(self.backup_collection_name)
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§å¤‡ä»½: {self.backup_collection_name}")
            
            # é‡å‘½åå½“å‰é›†åˆä¸ºå¤‡ä»½
            if utility.has_collection(self.collection_name):
                # ç”±äºMilvusä¸æ”¯æŒç›´æ¥é‡å‘½åï¼Œæˆ‘ä»¬åˆ›å»ºæ–°é›†åˆå¹¶å¤åˆ¶æ•°æ®
                original_collection = Collection(self.collection_name)
                original_collection.load()
                
                # è·å–æ‰€æœ‰æ•°æ®
                all_data = original_collection.query(
                    expr="id >= 0",
                    output_fields=["*"],
                    limit=100000
                )
                
                print(f"âœ… å¤‡ä»½å®Œæˆï¼Œå…± {len(all_data)} æ¡è®°å½•")
                return True
            else:
                print(f"âŒ åŸé›†åˆ {self.collection_name} ä¸å­˜åœ¨")
                return False
                
        except Exception as e:
            print(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def extract_and_group_content(self) -> Dict[str, List[Dict]]:
        """æå–å¹¶æŒ‰URLåˆ†ç»„å†…å®¹"""
        try:
            print("ğŸ“Š æå–ç°æœ‰æ•°æ®...")
            collection = Collection(self.collection_name)
            collection.load()
            
            # è·å–æ‰€æœ‰æ•°æ®
            all_data = collection.query(
                expr="id >= 0",
                output_fields=["*"],
                limit=100000
            )
            
            # æŒ‰URLåˆ†ç»„
            grouped_data = defaultdict(list)
            for item in all_data:
                url = item.get('url', 'unknown')
                grouped_data[url].append(item)
            
            print(f"âœ… æå–å®Œæˆï¼Œå…± {len(all_data)} æ¡è®°å½•ï¼Œæ¥è‡ª {len(grouped_data)} ä¸ªURL")
            return dict(grouped_data)
            
        except Exception as e:
            print(f"âŒ æå–æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def reprocess_grouped_content(self, grouped_data: Dict[str, List[Dict]]) -> List[Tuple[str, Dict]]:
        """é‡æ–°å¤„ç†åˆ†ç»„åçš„å†…å®¹"""
        print("ğŸ”„ é‡æ–°å¤„ç†å†…å®¹...")
        
        reprocessed_items = []
        
        for url, items in grouped_data.items():
            print(f"å¤„ç†URL: {url} (åŒ…å« {len(items)} ä¸ªç‰‡æ®µ)")
            
            # æŒ‰content_typeåˆ†ç»„ï¼Œç„¶ååˆå¹¶åŒç±»å‹å†…å®¹
            type_groups = defaultdict(list)
            for item in items:
                content_type = item.get('content_type', 'text')
                type_groups[content_type].append(item.get('content', ''))
            
            # ä¸ºæ¯ä¸ªå†…å®¹ç±»å‹åˆ›å»ºåˆå¹¶çš„æ–‡æœ¬
            for content_type, contents in type_groups.items():
                if not contents:
                    continue
                
                # åˆå¹¶å†…å®¹ï¼Œå»é‡
                merged_content = ' '.join(contents)
                if len(merged_content.strip()) < 50:  # è·³è¿‡å¤ªçŸ­çš„å†…å®¹
                    continue
                
                # ä½¿ç”¨æ™ºèƒ½åˆ†å—
                metadata = {
                    'url': url,
                    'content_type': content_type,
                    'title': items[0].get('title', ''),
                    'original_count': len(contents)
                }
                
                chunks = self.chunker.chunk_content(merged_content, metadata)
                
                # è½¬æ¢ä¸ºå­˜å‚¨æ ¼å¼
                for chunk_text, chunk_metadata in chunks:
                    processed_item = {
                        'content': chunk_text,
                        'url': url,
                        'title': metadata['title'],
                        'content_type': f"{content_type}_smart",
                        'chunk_index': chunk_metadata.chunk_index,
                        'total_chunks': chunk_metadata.total_chunks,
                        'quality_score': chunk_metadata.quality_score,
                        'word_count': chunk_metadata.word_count,
                        'language': chunk_metadata.language
                    }
                    reprocessed_items.append((chunk_text, processed_item))
        
        print(f"âœ… é‡æ–°å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(reprocessed_items)} ä¸ªæ™ºèƒ½å—")
        return reprocessed_items
    
    def create_new_collection(self, reprocessed_items: List[Tuple[str, Dict]]) -> bool:
        """åˆ›å»ºæ–°çš„ä¼˜åŒ–é›†åˆ"""
        try:
            print("ğŸ—ï¸  åˆ›å»ºæ–°çš„ä¼˜åŒ–é›†åˆ...")
            
            # åˆ é™¤ç°æœ‰çš„æ–°é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if utility.has_collection(self.new_collection_name):
                utility.drop_collection(self.new_collection_name)
                print(f"ğŸ—‘ï¸  åˆ é™¤ç°æœ‰é›†åˆ: {self.new_collection_name}")
            
            # è¿æ¥å¤„ç†å™¨å¹¶åˆ›å»ºé›†åˆ
            if not self.processor.connect():
                print("âŒ æ— æ³•è¿æ¥åˆ°Milvus")
                return False
            
            if not self.processor.create_collection():
                print("âŒ åˆ›å»ºé›†åˆå¤±è´¥")
                return False
            
            # æ‰¹é‡æ’å…¥æ•°æ®
            batch_size = 100
            inserted_count = 0
            
            for i in range(0, len(reprocessed_items), batch_size):
                batch = reprocessed_items[i:i+batch_size]
                
                # å‡†å¤‡æ‰¹é‡æ•°æ®
                batch_data = []
                for content, metadata in batch:
                    # å‘é‡åŒ–å†…å®¹
                    vector = self.processor.text_to_vector(content)
                    if vector:
                        data_point = {
                            'id': inserted_count,
                            'content': content,
                            'vector': vector,
                            'url': metadata['url'],
                            'title': metadata['title'],
                            'content_type': metadata['content_type'],
                            'chunk_index': metadata.get('chunk_index', 0),
                            'quality_score': metadata.get('quality_score', 0.0),
                            'word_count': metadata.get('word_count', 0),
                            'language': metadata.get('language', 'unknown')
                        }
                        batch_data.append(data_point)
                        inserted_count += 1
                
                # æ’å…¥æ‰¹é‡æ•°æ®
                if batch_data:
                    entities = [
                        [item['id'] for item in batch_data],
                        [item['content'] for item in batch_data],
                        [item['vector'] for item in batch_data],
                        [item['url'] for item in batch_data],
                        [item['title'] for item in batch_data],
                        [item['content_type'] for item in batch_data]
                    ]
                    
                    self.processor.collection.insert(entities)
                    print(f"ğŸ“¥ å·²æ’å…¥ {len(batch_data)} æ¡è®°å½• (æ€»è®¡: {inserted_count})")
            
            # åˆ›å»ºç´¢å¼•
            print("ğŸ”— åˆ›å»ºç´¢å¼•...")
            self.processor.create_index()
            
            print(f"âœ… æ–°é›†åˆåˆ›å»ºå®Œæˆï¼Œå…±æ’å…¥ {inserted_count} æ¡è®°å½•")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ–°é›†åˆå¤±è´¥: {e}")
            return False
        finally:
            self.processor.disconnect()
    
    def run_reprocessing(self):
        """æ‰§è¡Œå®Œæ•´çš„é‡æ–°å¤„ç†æµç¨‹"""
        print("ğŸš€ å¼€å§‹æ™ºèƒ½åˆ†å—é‡æ–°å¤„ç†")
        print("=" * 60)
        
        # 1. å¤‡ä»½ç°æœ‰æ•°æ®
        if not self.backup_existing_data():
            print("âŒ å¤‡ä»½å¤±è´¥ï¼Œåœæ­¢å¤„ç†")
            return False
        
        # 2. æå–å¹¶åˆ†ç»„å†…å®¹
        grouped_data = self.extract_and_group_content()
        if not grouped_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œåœæ­¢å¤„ç†")
            return False
        
        # 3. é‡æ–°å¤„ç†å†…å®¹
        reprocessed_items = self.reprocess_grouped_content(grouped_data)
        if not reprocessed_items:
            print("âŒ é‡æ–°å¤„ç†å¤±è´¥ï¼Œåœæ­¢å¤„ç†")
            return False
        
        # 4. åˆ›å»ºæ–°é›†åˆ
        if not self.create_new_collection(reprocessed_items):
            print("âŒ åˆ›å»ºæ–°é›†åˆå¤±è´¥")
            return False
        
        print("\nğŸ‰ æ™ºèƒ½åˆ†å—é‡æ–°å¤„ç†å®Œæˆï¼")
        print(f"æ–°é›†åˆåç§°: {self.new_collection_name}")
        print("\nğŸ“ åç»­æ­¥éª¤:")
        print(f"1. æµ‹è¯•æ–°é›†åˆ: {self.new_collection_name}")
        print(f"2. å¦‚æœæ»¡æ„ï¼Œå¯ä»¥å°† web_app.py ä¸­çš„é›†åˆåæ”¹ä¸º: {self.new_collection_name}")
        print(f"3. å¤‡ä»½æ•°æ®åœ¨åŸé›†åˆä¸­ï¼Œå¯ä»¥éšæ—¶æ¢å¤")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– æ™ºèƒ½åˆ†å—æ•°æ®é‡æ–°å¤„ç†å·¥å…·")
    print("æ­¤å·¥å…·å°†é‡æ–°ç»„ç»‡ç°æœ‰çš„ç»†ç²’åº¦æ•°æ®ä¸ºæ›´æœ‰æ„ä¹‰çš„å—")
    print()
    
    # ç¡®è®¤æ“ä½œ
    confirm = input("âš ï¸  è¿™å°†åˆ›å»ºæ–°é›†åˆå¹¶é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œç»§ç»­ï¼Ÿ(y/N): ").strip().lower()
    if confirm != 'y':
        print("ğŸš« æ“ä½œå·²å–æ¶ˆ")
        return
    
    # æ‰§è¡Œé‡æ–°å¤„ç†
    reprocessor = DataReprocessor()
    success = reprocessor.run_reprocessing()
    
    if success:
        print("\nâœ… å¤„ç†æˆåŠŸå®Œæˆ")
    else:
        print("\nâŒ å¤„ç†å¤±è´¥")

if __name__ == "__main__":
    main()